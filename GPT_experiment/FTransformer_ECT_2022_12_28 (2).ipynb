{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04D_jHiClylt"
      },
      "source": [
        "\n",
        "# Data Reading Prepration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiAZIbRFsIBk",
        "outputId": "02f6b17c-1eeb-4100-f22c-c1a4c3a7c6bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "from dateutil.parser import parse\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import matplotlib\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiALauuH_5Gx",
        "outputId": "c328987e-6a89-4c93-e94c-5474911b377e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8mO4cRvpzjJU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCH0GJ3cs4Qq"
      },
      "outputs": [],
      "source": [
        "# #id\n",
        "# %cd gdrive/MyDrive/Sajjad_cloned_ready\n",
        "# !unzip 'Sajjad cloned_id.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6b4Xz96tDov"
      },
      "outputs": [],
      "source": [
        "# #2_channels\n",
        "# %cd gdrive/MyDrive/Sajjad_cloned_ready\n",
        "# !unzip 2_Channels.zip\n",
        "# %cd Données-Copie-Sept 2020_2_channel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "%cd /content/gdrive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-VD70-csjqa",
        "outputId": "08e1691b-9dc0-4860-a354-5160d43a42f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEpCGLFwrA72"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/joseffaghihi/Causal-fuzzy-Transformers.git\n",
        "%cd Causal-fuzzy-Transformers/GPT_experiment\n",
        "!unzip transformers-main.zip\n",
        "%cd transformers-main\n",
        "!pip install /content/Causal-fuzzy-Transformers/GPT_experiment/transformers-main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/Causal-fuzzy-Transformers/GPT_experiment/transformers-main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6leoFYGCzNVK",
        "outputId": "7263bc8c-fc1e-43ee-bf81-818c88490a49"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/Causal-fuzzy-Transformers/GPT_experiment/transformers-main\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0.dev0) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0.dev0) (1.22.4)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0.dev0) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0.dev0) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.24.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.24.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.24.0.dev0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.24.0.dev0) (2022.12.7)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.24.0.dev0-py3-none-any.whl size=5486219 sha256=63b9eadcce3edbba34f24d0d94f246df2e2984c02a71a10bede0b454cb5331e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/51/10/9b870be45a989ab550e83cda8ea5f2dd9adce6e2c9c85d8894\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.24.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIUDAbfhqmIa"
      },
      "source": [
        "# Data Reading Body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7EjEDM-I4Tda"
      },
      "outputs": [],
      "source": [
        "#Extract the information of an EEG file from the excel file\n",
        "def is_date(string, fuzzy=False):\n",
        "   \n",
        "    try: \n",
        "        parse(string, fuzzy=fuzzy)\n",
        "        return True\n",
        "\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def Date(List):\n",
        "  for i in List:\n",
        "    if is_date(i):\n",
        "      return i\n",
        "          \n",
        "def get_patient_id(Str):\n",
        "\n",
        "  for i in nltk.tokenize.wordpunct_tokenize(Str):\n",
        "    if i.isdigit():\n",
        "        if len(i)==6:\n",
        "          return(i)\n",
        "        elif len(i)>6:\n",
        "          print('The Id number is incorrect for  file name:',Str) \n",
        "          print(\"Please make shure that the patient's id should be a six digits number\")      \n",
        "\n",
        "               \n",
        "def EEG_reader(path):\n",
        "  a = pd.read_fwf(path , skiprows=2, header=None, encoding='ISO-8859-1').drop(0, axis=0)\n",
        "  b = pd.read_csv(path , skiprows=2, header=None, sep='\\t', encoding='ISO-8859-1').drop(0, axis=0).drop(0, axis=1)\n",
        "  if a.shape[1]>1:\n",
        "    return a\n",
        "  if b.shape[1]>1:\n",
        "    return b\n",
        "  if a.shape[1] + b.shape[1]<=2:\n",
        "    raise Exception('The EEG file at {} has a shape or encoding error. make sure the encoding is Utf-8'.format(path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f9K-ulX3qdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dfaf94f-a2f0-4748-861f-7530083b7e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 541/541 [08:53<00:00,  1.01it/s]\n",
            "100%|██████████| 43/43 [00:16<00:00,  2.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Excel files' name  have duplicates\n",
            "['135866.xlsx', '170894.xlsx', '191007.xlsx', '201575.xlsx', '203961.xlsx', 'Fichier patient-212629.xlsx', '218132.xlsx', '218957.xlsx', '226238.xlsx', '227193.xlsx', '234300.xlsx', '245446.xlsx', '258322.xlsx', 'Fichier patient-289577.xlsx', '296707.xlsx', '325733.xlsx', '334217.xlsx', '342203.xlsx', '342303.xlsx', '348655.xlsx', '353456.xlsx', '354886.xlsx', 'Fichier patient-363269.xlsx', '369690.xlsx', 'Fichier patient-374227.xlsx', '377765.xlsx', '388304.xlsx', 'Fichier patient-399426.xlsx', 'Fichier patient-419494.xlsx', '420874.xlsx', '440246.xlsx', '465293.xlsx', '465293.xlsx', '491130.xlsx', '494811.xlsx', '514538.xlsx', '526847.xlsx', '537723.xlsx', 'Fichier patient-615502.xlsx', '640178.xlsx', '674119.xlsx', '685561.xlsx', 'Fichier patient.xlsx']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def cols(path):\n",
        "  a = list(pd.read_fwf(path , header=None).drop(0, axis=0).iloc[1].values)\n",
        "  if 'EEG0' in a or 'EEG1' in a or 'EEG2' in a or 'EEG3' in a:\n",
        "    for i in a:\n",
        "      if i.__class__ != str:\n",
        "        a.remove(i)\n",
        "    if a[-1].__class__ != str:\n",
        "        a.remove(a[-1])     \n",
        "    return a\n",
        "  else:\n",
        "    b= list(pd.read_csv(path,  header=None, sep='\\t').drop(0, axis=0).drop(0, axis=1).iloc[0].values)\n",
        "    if 'EEG0' in b or 'EEG1' in b or 'EEG2' in b or 'EEG3' in b:\n",
        "      for i in b:\n",
        "        if i.__class__ != str:\n",
        "          b.remove(i)\n",
        "      if b[-1].__class__ != str:\n",
        "        b.remove(b[-1])      \n",
        "      return b \n",
        "    else:\n",
        "      raise ValueError('File in path: {} column is corrupted'.format(path)) \n",
        "# Writes the informention from excel file along with the EEG raw data into a dataframe.\n",
        "# itretates over the whole EEG folders\n",
        "n =30 # Number of EEG data folders\n",
        "EEGs= []  # Will be the EEG stack\n",
        "# cols = ['EEG1', 'EEG1', 'EEG2', 'EEG4', 'Site', 'Phase', 'Durée clinique ', 'Durée EEG', 'Qualité clinique',\n",
        "#  'Adranergie', 'Qualité Aplatissement', 'Patho', 'Age', 'Sexe', 'Charge', 'Ti vs Tt', 'Anesthésiant', 'Qualité EEG']\n",
        "# cols1 = ['EEG1', 'EEG1', 'Site', 'Phase', 'Durée clinique ', 'Durée EEG', 'Qualité clinique',\n",
        "#  'Adranergie', 'Qualité Aplatissement', 'Patho', 'Age', 'Sexe', 'Charge', 'Ti vs Tt', 'Anesthésiant', 'Qualité EEG'] \n",
        "\n",
        "path_txt = glob.iglob('/content/gdrive/My Drive/Sajjad_cloned_ready/**/*.txt', recursive=True)\n",
        "path_xls = glob.iglob('/content/gdrive/My Drive/Sajjad_cloned_ready/**/*.xlsx', recursive=True)\n",
        "Path_xls = []\n",
        "Path_txt = []\n",
        "Txt = []\n",
        "Xls = []\n",
        "for i in path_txt:\n",
        "  Path_txt.append(i)\n",
        "for i in path_xls:\n",
        "  Path_xls.append(i) \n",
        "for i in tqdm(Path_txt):\n",
        "  try:\n",
        "    if EEG_reader(i).shape[0]>1:\n",
        "      Txt.append(EEG_reader(i))\n",
        "  except:\n",
        "    pass    \n",
        "for i in tqdm(Path_xls):\n",
        "  Xls.append((i,pd.read_excel(i)))\n",
        "  \n",
        "\n",
        "duplicate = []\n",
        "duplicate_names = None\n",
        "for  i in range(len(Path_xls)):\n",
        "  duplicate.append(Path_xls[i].split('/')[-1])\n",
        "if len(duplicate)>=2:\n",
        "  duplicate_names = True\n",
        "else:\n",
        "  duplicate_names = False\n",
        "\n",
        "\"___________________________________________________________________________________________\"  \n",
        "\"___________________________________________________________________________________________\"  \n",
        "if duplicate_names == False:\n",
        "  EEGs=[]\n",
        "  c0=0\n",
        "  c1=0\n",
        "  for j in tqdm(Path_txt):\n",
        "    for i in Xls: \n",
        "      try:\n",
        "        Site = i[1] [i[1]['Date']==Date(j.split(',')).strip()]\n",
        "      except:\n",
        "        pass  \n",
        "      if len(Site)>=1:\n",
        "        EEG = Txt[Path_txt.index(j)]\n",
        "        Site1 = Site.values\n",
        "        try:\n",
        "          for k in Site1:\n",
        "            EEGs.append(pd.DataFrame(np.hstack([EEG.values, \n",
        "                                                np.vstack([k for i in range(EEG.shape[0])])]),columns=[*cols(j), *list(Site.columns)]))\n",
        "            \n",
        "        except:\n",
        "          c1=Site \n",
        "\n",
        "\"___________________________________________________________________________________________\"  \n",
        "\"___________________________________________________________________________________________\"   \n",
        "if duplicate_names == True:\n",
        "  print (\"The Excel files' name  have duplicates\")\n",
        "  print(duplicate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a complete list of EEG2 channels and 4 channels with patients' information . "
      ],
      "metadata": {
        "id": "TieIyXhesieP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S6qiBJosO4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafb020d-49e0-405e-f392-5ef353c5996a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 97/541 [00:13<01:03,  7.02it/s]<ipython-input-6-73a3f4df2c14>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  mat['Patient_id'] = get_patient_id(i[0])\n",
            "<ipython-input-6-73a3f4df2c14>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  mat['Xls_file_path'] = i[0]\n",
            " 40%|███▉      | 216/541 [00:34<01:01,  5.26it/s]<ipython-input-6-73a3f4df2c14>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  mat['EEG_file_path'] = j\n",
            "100%|██████████| 541/541 [01:29<00:00,  6.02it/s]\n"
          ]
        }
      ],
      "source": [
        "EEGs_Quad_Channel=[]\n",
        "EEGs_Dual_Channel=[]\n",
        "Outlyers=[]\n",
        "c0=0\n",
        "c1=0\n",
        "for j in tqdm(Path_txt):\n",
        "  for i in Xls: \n",
        "    try:\n",
        "      Site = i[1][i[1]['Date']==Date(j.split(',')).strip()]\n",
        "    except:\n",
        "      pass  \n",
        "    if get_patient_id(j)==get_patient_id(i[0]):\n",
        "      try:\n",
        "        EEG = Txt[Path_txt.index(j)]\n",
        "        \n",
        "      except:\n",
        "        pass  \n",
        "      for k in Site.values:\n",
        "        mat = pd.DataFrame(np.hstack([EEG.values, np.vstack([k for i in range(EEG.shape[0])])]),columns=[*['EEG{}'.format(i) for i in range(EEG.shape[1])], *list(Site.columns)]).dropna(axis = 1, how = 'all')\n",
        "        mat['Patient_id'] = get_patient_id(i[0])\n",
        "        mat['Xls_file_path'] = i[0] \n",
        "        mat['EEG_file_path'] = j\n",
        "        try:                                    \n",
        "          Sexe = i[1]['Sexe'].iloc[0]\n",
        "          Age = i[1]['Age'].iloc[0]\n",
        "          Patho = i[1]['Patho'].iloc[0]\n",
        "          mat['Sexe'].iloc[0] = Sexe\n",
        "          mat['Age'].iloc[0] = Age\n",
        "          mat['Patho'].iloc[0] = Patho\n",
        "          \n",
        "        except:\n",
        "          pass \n",
        "          \n",
        "        if 'EEG1'in list(mat.columns) and 'EEG2'in list(mat.columns) and 'EEG3'  in list(mat.columns):\n",
        "          EEGs_Quad_Channel.append(mat)\n",
        "        if 'EEG0' in list(mat.columns) and 'EEG1' in list(mat.columns) and 'EEG2' not in list(mat.columns):\n",
        "          EEGs_Dual_Channel.append(mat)\n",
        "        if 'EEG1' in list(mat.columns) and 'EEG2' in list(mat.columns) and 'EEG3' not in list(mat.columns):\n",
        "          EEGs_Dual_Channel.append(mat)\n",
        "        if 'EEG0'not in list(mat.columns) and 'EEG1'not in list(mat.columns) and 'EEG2' not in list(mat.columns):\n",
        "          Outlyers.append(mat) \n",
        "        if 'ECG'in list(mat.columns):\n",
        "          Outlyers.append(mat)    \n",
        "\n",
        "         \n",
        "        \n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(EEGs_Quad_Channel), len(EEGs_Dual_Channel))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMdQB4LNPPqe",
        "outputId": "ae16e341-719b-43cc-f4b3-6c142c03dc10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "486 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EEGs_Quad_Channel[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "zT-LbdxQuLTF",
        "outputId": "0617cf9d-3c5e-4f39-e76d-30fd583c4cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        EEG0   EEG1   EEG2  EEG3        Date Ti vs Tt Site      Phase  \\\n",
              "0      -21.3   -5.8  -11.0  -1.3  23-02-2022       Tt  BiT  Entretien   \n",
              "1      -15.1   -1.8  -10.0  -4.3  23-02-2022       Tt  BiT  Entretien   \n",
              "2       -8.9    3.6   -8.9  -4.1  23-02-2022       Tt  BiT  Entretien   \n",
              "3       -9.8    4.0   -9.1   0.8  23-02-2022       Tt  BiT  Entretien   \n",
              "4      -15.3   -1.5  -11.4   6.8  23-02-2022       Tt  BiT  Entretien   \n",
              "...      ...    ...    ...   ...         ...      ...  ...        ...   \n",
              "35341  -16.1  -14.9   -2.9   2.6  23-02-2022       Tt  BiT  Entretien   \n",
              "35342  -14.3  -13.6   -2.6   2.9  23-02-2022       Tt  BiT  Entretien   \n",
              "35343  -12.5  -12.1   -2.9   3.4  23-02-2022       Tt  BiT  Entretien   \n",
              "35344  -11.1  -10.8   -3.5   3.6  23-02-2022       Tt  BiT  Entretien   \n",
              "35345  -10.3   -9.8   -4.1   3.3  23-02-2022       Tt  BiT  Entretien   \n",
              "\n",
              "      % Charge Durée clinique  Durée EEG Anesthésiant Qualité clinique  \\\n",
              "0           60             ###        56        Metho              +++   \n",
              "1           60             ###        56        Metho              +++   \n",
              "2           60             ###        56        Metho              +++   \n",
              "3           60             ###        56        Metho              +++   \n",
              "4           60             ###        56        Metho              +++   \n",
              "...        ...             ...       ...          ...              ...   \n",
              "35341       60             ###        56        Metho              +++   \n",
              "35342       60             ###        56        Metho              +++   \n",
              "35343       60             ###        56        Metho              +++   \n",
              "35344       60             ###        56        Metho              +++   \n",
              "35345       60             ###        56        Metho              +++   \n",
              "\n",
              "      Qualité EEG Adranergie Qualité Aplatissement Patient_id  \\\n",
              "0             +++          +                   +++     135866   \n",
              "1             +++          +                   +++     135866   \n",
              "2             +++          +                   +++     135866   \n",
              "3             +++          +                   +++     135866   \n",
              "4             +++          +                   +++     135866   \n",
              "...           ...        ...                   ...        ...   \n",
              "35341         +++          +                   +++     135866   \n",
              "35342         +++          +                   +++     135866   \n",
              "35343         +++          +                   +++     135866   \n",
              "35344         +++          +                   +++     135866   \n",
              "35345         +++          +                   +++     135866   \n",
              "\n",
              "                                           Xls_file_path  \\\n",
              "0      /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "1      /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "2      /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "3      /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "4      /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "...                                                  ...   \n",
              "35341  /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "35342  /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "35343  /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "35344  /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "35345  /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "\n",
              "                                           EEG_file_path  \n",
              "0      /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "1      /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "2      /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "3      /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "4      /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "...                                                  ...  \n",
              "35341  /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "35342  /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "35343  /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "35344  /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "35345  /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "\n",
              "[35346 rows x 19 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-04d326cb-26a6-4176-ace5-b4cf54ab32aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EEG0</th>\n",
              "      <th>EEG1</th>\n",
              "      <th>EEG2</th>\n",
              "      <th>EEG3</th>\n",
              "      <th>Date</th>\n",
              "      <th>Ti vs Tt</th>\n",
              "      <th>Site</th>\n",
              "      <th>Phase</th>\n",
              "      <th>% Charge</th>\n",
              "      <th>Durée clinique</th>\n",
              "      <th>Durée EEG</th>\n",
              "      <th>Anesthésiant</th>\n",
              "      <th>Qualité clinique</th>\n",
              "      <th>Qualité EEG</th>\n",
              "      <th>Adranergie</th>\n",
              "      <th>Qualité Aplatissement</th>\n",
              "      <th>Patient_id</th>\n",
              "      <th>Xls_file_path</th>\n",
              "      <th>EEG_file_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-21.3</td>\n",
              "      <td>-5.8</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>-1.3</td>\n",
              "      <td>23-02-2022</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>60</td>\n",
              "      <td>###</td>\n",
              "      <td>56</td>\n",
              "      <td>Metho</td>\n",
              "      <td>+++</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>+++</td>\n",
              "      <td>135866</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-15.1</td>\n",
              "      <td>-1.8</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-4.3</td>\n",
              "      <td>23-02-2022</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>60</td>\n",
              "      <td>###</td>\n",
              "      <td>56</td>\n",
              "      <td>Metho</td>\n",
              "      <td>+++</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>+++</td>\n",
              "      <td>135866</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-8.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>-8.9</td>\n",
              "      <td>-4.1</td>\n",
              "      <td>23-02-2022</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>60</td>\n",
              "      <td>###</td>\n",
              "      <td>56</td>\n",
              "      <td>Metho</td>\n",
              "      <td>+++</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>+++</td>\n",
              "      <td>135866</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-9.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-9.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>23-02-2022</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>60</td>\n",
              "      <td>###</td>\n",
              "      <td>56</td>\n",
              "      <td>Metho</td>\n",
              "      <td>+++</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>+++</td>\n",
              "      <td>135866</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-15.3</td>\n",
              "      <td>-1.5</td>\n",
              "      <td>-11.4</td>\n",
              "      <td>6.8</td>\n",
              "      <td>23-02-2022</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>60</td>\n",
              "      <td>###</td>\n",
              "      <td>56</td>\n",
              "      <td>Metho</td>\n",
              "      <td>+++</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>+++</td>\n",
              "      <td>135866</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35341</th>\n",
              "      <td>-16.1</td>\n",
              "      <td>-14.9</td>\n",
              "      <td>-2.9</td>\n",
              "      <td>2.6</td>\n",
              "      <td>23-02-2022</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>60</td>\n",
              "      <td>###</td>\n",
              "      <td>56</td>\n",
              "      <td>Metho</td>\n",
              "      <td>+++</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>+++</td>\n",
              "      <td>135866</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35342</th>\n",
              "      <td>-14.3</td>\n",
              "      <td>-13.6</td>\n",
              "      <td>-2.6</td>\n",
              "      <td>2.9</td>\n",
              "      <td>23-02-2022</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>60</td>\n",
              "      <td>###</td>\n",
              "      <td>56</td>\n",
              "      <td>Metho</td>\n",
              "      <td>+++</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>+++</td>\n",
              "      <td>135866</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35343</th>\n",
              "      <td>-12.5</td>\n",
              "      <td>-12.1</td>\n",
              "      <td>-2.9</td>\n",
              "      <td>3.4</td>\n",
              "      <td>23-02-2022</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>60</td>\n",
              "      <td>###</td>\n",
              "      <td>56</td>\n",
              "      <td>Metho</td>\n",
              "      <td>+++</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>+++</td>\n",
              "      <td>135866</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35344</th>\n",
              "      <td>-11.1</td>\n",
              "      <td>-10.8</td>\n",
              "      <td>-3.5</td>\n",
              "      <td>3.6</td>\n",
              "      <td>23-02-2022</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>60</td>\n",
              "      <td>###</td>\n",
              "      <td>56</td>\n",
              "      <td>Metho</td>\n",
              "      <td>+++</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>+++</td>\n",
              "      <td>135866</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35345</th>\n",
              "      <td>-10.3</td>\n",
              "      <td>-9.8</td>\n",
              "      <td>-4.1</td>\n",
              "      <td>3.3</td>\n",
              "      <td>23-02-2022</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>60</td>\n",
              "      <td>###</td>\n",
              "      <td>56</td>\n",
              "      <td>Metho</td>\n",
              "      <td>+++</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>+++</td>\n",
              "      <td>135866</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>35346 rows × 19 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04d326cb-26a6-4176-ace5-b4cf54ab32aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-04d326cb-26a6-4176-ace5-b4cf54ab32aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-04d326cb-26a6-4176-ace5-b4cf54ab32aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EEGs_Dual_Channel[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "Ya1-io_iSp4g",
        "outputId": "e6e4270e-8e96-49ee-f33d-f52a443d06aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       EEG0  EEG1        Date Ti vs Tt Site      Phase % Charge  \\\n",
              "0     -16.1  12.5  09-10-2019       Tt  BiT  Entretien       95   \n",
              "1     -13.6  14.0  09-10-2019       Tt  BiT  Entretien       95   \n",
              "2     -12.0  13.9  09-10-2019       Tt  BiT  Entretien       95   \n",
              "3     -11.8  11.1  09-10-2019       Tt  BiT  Entretien       95   \n",
              "4     -12.9   6.4  09-10-2019       Tt  BiT  Entretien       95   \n",
              "...     ...   ...         ...      ...  ...        ...      ...   \n",
              "17186   0.4  -0.1  09-10-2019       Tt  BiT  Entretien       95   \n",
              "17187  -0.1  -0.1  09-10-2019       Tt  BiT  Entretien       95   \n",
              "17188  -0.8  -0.4  09-10-2019       Tt  BiT  Entretien       95   \n",
              "17189  -1.6  -0.9  09-10-2019       Tt  BiT  Entretien       95   \n",
              "17190  -2.5  -1.5  09-10-2019       Tt  BiT  Entretien       95   \n",
              "\n",
              "      Durée clinique  Durée EEG  Anesthésiant Qualité clinique Qualité EEG  \\\n",
              "0                   0        25  Methohexital                O         +++   \n",
              "1                   0        25  Methohexital                O         +++   \n",
              "2                   0        25  Methohexital                O         +++   \n",
              "3                   0        25  Methohexital                O         +++   \n",
              "4                   0        25  Methohexital                O         +++   \n",
              "...               ...       ...           ...              ...         ...   \n",
              "17186               0        25  Methohexital                O         +++   \n",
              "17187               0        25  Methohexital                O         +++   \n",
              "17188               0        25  Methohexital                O         +++   \n",
              "17189               0        25  Methohexital                O         +++   \n",
              "17190               0        25  Methohexital                O         +++   \n",
              "\n",
              "      Adranergie Qualité Aplatissement        Patho Patient_id  \\\n",
              "0              +                    ++  EDM anxieux     191007   \n",
              "1              +                    ++  EDM anxieux     191007   \n",
              "2              +                    ++  EDM anxieux     191007   \n",
              "3              +                    ++  EDM anxieux     191007   \n",
              "4              +                    ++  EDM anxieux     191007   \n",
              "...          ...                   ...          ...        ...   \n",
              "17186          +                    ++  EDM anxieux     191007   \n",
              "17187          +                    ++  EDM anxieux     191007   \n",
              "17188          +                    ++  EDM anxieux     191007   \n",
              "17189          +                    ++  EDM anxieux     191007   \n",
              "17190          +                    ++  EDM anxieux     191007   \n",
              "\n",
              "                                           Xls_file_path  \\\n",
              "0      /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "1      /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "2      /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "3      /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "4      /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "...                                                  ...   \n",
              "17186  /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "17187  /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "17188  /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "17189  /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "17190  /content/gdrive/My Drive/Sajjad_cloned_ready/N...   \n",
              "\n",
              "                                           EEG_file_path  \n",
              "0      /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "1      /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "2      /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "3      /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "4      /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "...                                                  ...  \n",
              "17186  /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "17187  /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "17188  /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "17189  /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "17190  /content/gdrive/My Drive/Sajjad_cloned_ready/N...  \n",
              "\n",
              "[17191 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d7473bf4-16aa-4581-a71d-a11272a87e32\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EEG0</th>\n",
              "      <th>EEG1</th>\n",
              "      <th>Date</th>\n",
              "      <th>Ti vs Tt</th>\n",
              "      <th>Site</th>\n",
              "      <th>Phase</th>\n",
              "      <th>% Charge</th>\n",
              "      <th>Durée clinique</th>\n",
              "      <th>Durée EEG</th>\n",
              "      <th>Anesthésiant</th>\n",
              "      <th>Qualité clinique</th>\n",
              "      <th>Qualité EEG</th>\n",
              "      <th>Adranergie</th>\n",
              "      <th>Qualité Aplatissement</th>\n",
              "      <th>Patho</th>\n",
              "      <th>Patient_id</th>\n",
              "      <th>Xls_file_path</th>\n",
              "      <th>EEG_file_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-16.1</td>\n",
              "      <td>12.5</td>\n",
              "      <td>09-10-2019</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>Methohexital</td>\n",
              "      <td>O</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>++</td>\n",
              "      <td>EDM anxieux</td>\n",
              "      <td>191007</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-13.6</td>\n",
              "      <td>14.0</td>\n",
              "      <td>09-10-2019</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>Methohexital</td>\n",
              "      <td>O</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>++</td>\n",
              "      <td>EDM anxieux</td>\n",
              "      <td>191007</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-12.0</td>\n",
              "      <td>13.9</td>\n",
              "      <td>09-10-2019</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>Methohexital</td>\n",
              "      <td>O</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>++</td>\n",
              "      <td>EDM anxieux</td>\n",
              "      <td>191007</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-11.8</td>\n",
              "      <td>11.1</td>\n",
              "      <td>09-10-2019</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>Methohexital</td>\n",
              "      <td>O</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>++</td>\n",
              "      <td>EDM anxieux</td>\n",
              "      <td>191007</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-12.9</td>\n",
              "      <td>6.4</td>\n",
              "      <td>09-10-2019</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>Methohexital</td>\n",
              "      <td>O</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>++</td>\n",
              "      <td>EDM anxieux</td>\n",
              "      <td>191007</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17186</th>\n",
              "      <td>0.4</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>09-10-2019</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>Methohexital</td>\n",
              "      <td>O</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>++</td>\n",
              "      <td>EDM anxieux</td>\n",
              "      <td>191007</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17187</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>09-10-2019</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>Methohexital</td>\n",
              "      <td>O</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>++</td>\n",
              "      <td>EDM anxieux</td>\n",
              "      <td>191007</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17188</th>\n",
              "      <td>-0.8</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>09-10-2019</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>Methohexital</td>\n",
              "      <td>O</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>++</td>\n",
              "      <td>EDM anxieux</td>\n",
              "      <td>191007</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17189</th>\n",
              "      <td>-1.6</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>09-10-2019</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>Methohexital</td>\n",
              "      <td>O</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>++</td>\n",
              "      <td>EDM anxieux</td>\n",
              "      <td>191007</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17190</th>\n",
              "      <td>-2.5</td>\n",
              "      <td>-1.5</td>\n",
              "      <td>09-10-2019</td>\n",
              "      <td>Tt</td>\n",
              "      <td>BiT</td>\n",
              "      <td>Entretien</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>Methohexital</td>\n",
              "      <td>O</td>\n",
              "      <td>+++</td>\n",
              "      <td>+</td>\n",
              "      <td>++</td>\n",
              "      <td>EDM anxieux</td>\n",
              "      <td>191007</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "      <td>/content/gdrive/My Drive/Sajjad_cloned_ready/N...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17191 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7473bf4-16aa-4581-a71d-a11272a87e32')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d7473bf4-16aa-4581-a71d-a11272a87e32 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d7473bf4-16aa-4581-a71d-a11272a87e32');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG8kNYJ6dWSp"
      },
      "outputs": [],
      "source": [
        "# Preshock and postshock seperation and concatenation, delete during shock.\n",
        "\n",
        "LQuad = len(EEGs_Quad_Channel)\n",
        "cols = ['index', 'EEG0', 'EEG1', 'EEG2', 'EEG3','Post_EEG0', 'Post_EEG1', 'Post_EEG2', 'Post_EEG3', 'Date', 'Ti vs Tt', 'Site',\n",
        "       'Phase', '% Charge', 'Durée clinique ', 'Durée EEG', 'Anesthésiant',\n",
        "       'Qualité clinique', 'Qualité EEG', 'Adranergie',\n",
        "       'Qualité Aplatissement', 'Patient_id', 'Xls_file_path', 'EEG_file_path']\n",
        "#=============================================================================================#\n",
        "Pre_shock1 = [EEGs_Quad_Channel[i].iloc[0:min(EEGs_Quad_Channel[i]['EEG0'].astype(float)[EEGs_Quad_Channel[i]['EEG0'].astype(float) == EEGs_Quad_Channel[i]['EEG0'].astype(float).max()].index[0],\n",
        "                                              EEGs_Quad_Channel[i]['EEG1'].astype(float)[EEGs_Quad_Channel[i]['EEG1'].astype(float) == EEGs_Quad_Channel[i]['EEG1'].astype(float).max()].index[0],\n",
        "                                              EEGs_Quad_Channel[i]['EEG2'].astype(float)[EEGs_Quad_Channel[i]['EEG2'].astype(float) == EEGs_Quad_Channel[i]['EEG2'].astype(float).max()].index[0],\n",
        "                                              EEGs_Quad_Channel[i]['EEG3'].astype(float)[EEGs_Quad_Channel[i]['EEG3'].astype(float) == EEGs_Quad_Channel[i]['EEG3'].astype(float).max()].index[0])].reset_index() for i in range(LQuad)]\n",
        "Post_shock1 = [EEGs_Quad_Channel[i].iloc[-1000:-1].reset_index() for i in range(len(EEGs_Quad_Channel))]\n",
        "#=============================================================================================#\n",
        "\n",
        "for i in range(LQuad):\n",
        "  Pre_shock1[i]['Post_EEG0'] = Post_shock1[i]['EEG0'] \n",
        "  Pre_shock1[i]['Post_EEG1'] = Post_shock1[i]['EEG1'] \n",
        "  Pre_shock1[i]['Post_EEG2'] = Post_shock1[i]['EEG2'] \n",
        "  Pre_shock1[i]['Post_EEG3'] = Post_shock1[i]['EEG3'] \n",
        "  Pre_shock1[i] = Pre_shock1[i].reindex(columns = cols)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb_bYo8Cfr9z"
      },
      "outputs": [],
      "source": [
        "Pre_post_shock = pd.concat(Pre_shock1, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky4xfm1wILoL"
      },
      "source": [
        "Quad channel success rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "gg9-oW6U5ewH",
        "outputId": "95055a3b-143f-4378-ec26-0bf1709a9363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:128: MatplotlibDeprecationWarning: Support for uppercase single-letter colors is deprecated since Matplotlib 3.1 and will be removed in 3.3; please use lowercase instead.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "G =[]\n",
        "B =[]\n",
        "\n",
        "for j in range (100):\n",
        "  g = [Pre_shock1[i] for i in range(len(EEGs_Quad_Channel)) if Pre_shock1[i]['% Charge'].iloc[0]==j and Pre_shock1[i]['Qualité EEG'].iloc[0]=='+']\n",
        "  g1 = [Pre_shock1[i] for i in range(len(EEGs_Quad_Channel)) if Pre_shock1[i]['% Charge'].iloc[0]==j and Pre_shock1[i]['Qualité EEG'].iloc[0]=='++']\n",
        "  g2 = [Pre_shock1[i] for i in range(len(EEGs_Quad_Channel)) if Pre_shock1[i]['% Charge'].iloc[0]==j and Pre_shock1[i]['Qualité EEG'].iloc[0]=='+++']\n",
        "\n",
        "  b = [Pre_shock1[i] for i in range(len(EEGs_Quad_Channel)) if Pre_shock1[i]['% Charge'].iloc[0]==j and Pre_shock1[i]['Qualité EEG'].iloc[0]=='-']\n",
        "  G.append(len(g)+len(g1)+len(g2))\n",
        "  B.append(len(b))\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(G, color ='G')\n",
        "plt.plot(B, color ='R')  \n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Green were sucessfull and red unsucessfull, between 40- has 43sucess 45 has 55 sucess"
      ],
      "metadata": {
        "id": "uq3PiCPmv7Dl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "Lo9meUaNXraD",
        "outputId": "0bfb8ed3-14b9-4ceb-e779-970d59d85517"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "G =[]\n",
        "B =[]\n",
        "\n",
        "for j in range (100):\n",
        "  g = [Pre_shock1[i] for i in range(len(EEGs_Quad_Channel)) if Pre_shock1[i]['% Charge'].iloc[0]==j and Pre_shock1[i]['Qualité clinique'].iloc[0]=='+']\n",
        "  g1 = [Pre_shock1[i] for i in range(len(EEGs_Quad_Channel)) if Pre_shock1[i]['% Charge'].iloc[0]==j and Pre_shock1[i]['Qualité clinique'].iloc[0]=='++']\n",
        "  g2 = [Pre_shock1[i] for i in range(len(EEGs_Quad_Channel)) if Pre_shock1[i]['% Charge'].iloc[0]==j and Pre_shock1[i]['Qualité clinique'].iloc[0]=='+++']\n",
        "\n",
        "  b = [Pre_shock1[i] for i in range(len(EEGs_Quad_Channel)) if Pre_shock1[i]['% Charge'].iloc[0]==j and Pre_shock1[i]['Qualité clinique'].iloc[0]=='-']\n",
        "  G.append(len(g)+len(g1)+len(g2))\n",
        "  B.append(len(b))\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(G, color ='G')\n",
        "plt.plot(B, color ='R')  \n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLF6kEuljaKs"
      },
      "outputs": [],
      "source": [
        "Good_titration=[]\n",
        "Bad_titration=[]\n",
        "\n",
        "for i in Pre_shock1:\n",
        "  if i['% Charge'].iloc[0]==50 and i['Qualité EEG'].iloc[0]=='+++':\n",
        "    Good_titration.append(i)\n",
        "  if i['% Charge'].iloc[0]==50 and i['Qualité EEG'].iloc[0]=='-':\n",
        "    Bad_titration.append(i)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0svJCJon9Ixy",
        "outputId": "f3508f14-6d38-42d4-f269-16ced6332f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 27\n"
          ]
        }
      ],
      "source": [
        "print(len(Bad_titration),len(Good_titration))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot Fourrier "
      ],
      "metadata": {
        "id": "QWNVg6TewgXa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "Yol-CQtB6Ps_",
        "outputId": "d973a7d4-95a4-42ee-c5fa-cd521d19115b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.fftpack\n",
        "n=2\n",
        "# Number of sample points\n",
        "N = 128\n",
        "# sample spacing\n",
        "T = 1.0 / 100\n",
        "x = np.linspace(0.0, N*T, N)\n",
        "y = Good_titration[n]['EEG2'].values.astype(float)\n",
        "y1 = Bad_titration[n]['EEG2'].values.astype(float)\n",
        "yf = scipy.fftpack.fft(y)\n",
        "y1f = scipy.fftpack.fft(y1)\n",
        "xf = np.linspace(0.0, 1.0//(2.0*T), N//2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12,5))\n",
        "ax.bar(xf, 2.0/N * np.abs(yf[:N//2]),width=0.1, color ='g')\n",
        "ax.bar(xf, 2.0/N * np.abs(y1f[:N//2]),width=0.2 ,color ='orange')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vqQhlG86_y1"
      },
      "source": [
        "2022-05-31"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "rSSvk1EU687L",
        "outputId": "dbfe7cc4-6082-4253-a7a2-3ff4ac9669d4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.fftpack\n",
        "n=2\n",
        "m=4\n",
        "# Number of sample points\n",
        "N = 128\n",
        "# sample spacing\n",
        "T = 1.0 / 100\n",
        "x = np.linspace(0.0, N*T, N)\n",
        "y = Good_titration[n]['EEG2'].values.astype(float)\n",
        "y1 = Bad_titration[m]['EEG2'].values.astype(float)\n",
        "yf = scipy.fftpack.fft(y)\n",
        "y1f = scipy.fftpack.fft(y1)\n",
        "xf = np.linspace(0.0, 1.0//(2.0*T), N//2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12,5))\n",
        "ax.bar(xf, 2.0/N * np.abs(yf[:N//2]),width=0.1, color ='g')\n",
        "ax.bar(xf, 2.0/N * np.abs(y1f[:N//2]),width=0.2 ,color ='orange')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "E7iEI9LIlbgu",
        "outputId": "2e08d790-58dd-43be-a3c5-fead6c7ff91e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.fftpack\n",
        "n=7\n",
        "# Number of sample points\n",
        "N = 128\n",
        "# sample spacing\n",
        "T = 1.0 / 100\n",
        "x = np.linspace(0.0, N*T, N)\n",
        "y = Good_titration[n]['EEG0'].values.astype(float)\n",
        "y1 = Good_titration[0]['EEG0'].values.astype(float)\n",
        "yf = scipy.fftpack.fft(y)\n",
        "y1f = scipy.fftpack.fft(y1)\n",
        "xf = np.linspace(0.0, 1.0//(2.0*T), N//2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12,5))\n",
        "ax.bar(xf, 2.0/N * np.abs(yf[:N//2]),width=0.1, color ='g')\n",
        "ax.bar(xf, 2.0/N * np.abs(y1f[:N//2]),width=0.2 ,color ='orange')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "cg_2rd1NpiWF",
        "outputId": "597f4a4a-45f6-4c19-be81-8546e010bed7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.fftpack\n",
        "n=1\n",
        "# Number of sample points\n",
        "N = 128\n",
        "# sample spacing\n",
        "T = 1.0 / 100\n",
        "x = np.linspace(0.0, N*T, N)\n",
        "y = Good_titration[n]['EEG1'].values.astype(float)\n",
        "# y1 = Good_titration[0]['EEG1'].values.astype(float)\n",
        "yf = scipy.fftpack.fft(y)\n",
        "# y1f = scipy.fftpack.fft(y1)\n",
        "xf = np.linspace(0.0, 1.0//(2.0*T), N//2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12,5))\n",
        "ax.bar(xf, 2.0/N * np.abs(yf[:N//2]),width=0.1, color ='g')\n",
        "# ax.bar(xf, 2.0/N * np.abs(y1f[:N//2]),width=0.2 ,color ='orange')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2WewDEtps6s"
      },
      "outputs": [],
      "source": [
        "# for n in range(9):\n",
        "#   y = Bad_titration[n]['EEG1'].values.astype(float)\n",
        "#   yf = scipy.fftpack.fft(y)\n",
        "#   print(np.abs(yf[:N//2])[15]/64 )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44YvVnIe4u7W"
      },
      "source": [
        "Dual channel success rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2utPq5Vzx0nG"
      },
      "outputs": [],
      "source": [
        "# Preshock and postshock seperation and concatenation Dual channel\n",
        "\n",
        "cols = ['index', 'EEG0', 'EEG1','Post_EEG0', 'Post_EEG1', 'Date', 'Ti vs Tt', 'Site',\n",
        "       'Phase', '% Charge', 'Durée clinique ', 'Durée EEG', 'Anesthésiant',\n",
        "       'Qualité clinique', 'Qualité EEG', 'Adranergie',\n",
        "       'Qualité Aplatissement', 'Patient_id', 'Xls_file_path', 'EEG_file_path']\n",
        "#=============================================================================================#\n",
        "Pre_shock2 = [EEGs_Dual_Channel[i].iloc[0:min(EEGs_Dual_Channel[i]['EEG0'].astype(float)[EEGs_Dual_Channel[i]['EEG0'].astype(float) ==EEGs_Dual_Channel[i]['EEG0'].astype(float).max()].index[0],\n",
        "                                              EEGs_Dual_Channel[i]['EEG1'].astype(float)[EEGs_Dual_Channel[i]['EEG1'].astype(float) ==EEGs_Dual_Channel[i]['EEG1'].astype(float).max()].index[0])].reset_index() for i in range(len(EEGs_Dual_Channel))]\n",
        "Post_shock2 = [EEGs_Dual_Channel[i].iloc[-1000:-1].reset_index() for i in range(len(EEGs_Dual_Channel))]\n",
        "#=============================================================================================#\n",
        "\n",
        "for i in range(len(EEGs_Dual_Channel)):\n",
        "  Pre_shock2[i]['Post_EEG0'] = Post_shock2[i]['EEG0'] \n",
        "  Pre_shock2[i]['Post_EEG1'] = Post_shock2[i]['EEG1'] \n",
        "  Pre_shock2[i] = Pre_shock2[i].reindex(columns = cols)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "I2ELN4Wr1i_5",
        "outputId": "d0159df3-879c-4ae8-c240-8ba243e3c60d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "G =[]\n",
        "B =[]\n",
        "\n",
        "for j in range (100):\n",
        "  g = [Pre_shock2[i] for i in range(len(EEGs_Dual_Channel)) if Pre_shock2[i]['% Charge'].iloc[0]==j and Pre_shock2[i]['Qualité EEG'].iloc[0]=='+']\n",
        "  g1 = [Pre_shock2[i] for i in range(len(EEGs_Dual_Channel)) if Pre_shock2[i]['% Charge'].iloc[0]==j and Pre_shock2[i]['Qualité EEG'].iloc[0]=='++']\n",
        "  g2 = [Pre_shock2[i] for i in range(len(EEGs_Dual_Channel)) if Pre_shock2[i]['% Charge'].iloc[0]==j and Pre_shock2[i]['Qualité EEG'].iloc[0]=='+++']\n",
        "\n",
        "  b = [Pre_shock2[i] for i in range(len(EEGs_Dual_Channel)) if Pre_shock2[i]['% Charge'].iloc[0]==j and Pre_shock2[i]['Qualité EEG'].iloc[0]=='-']\n",
        "  G.append(len(g)+len(g1)+len(g2))\n",
        "  B.append(len(b))\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(G, color ='G')\n",
        "plt.plot(B, color ='R')  \n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80R_1-p13EKY"
      },
      "outputs": [],
      "source": [
        "Pre_post_shock2 = pd.concat(Pre_shock2, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-GDhGg0_jvp"
      },
      "source": [
        "EEGs Concatenation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UESg2pgfesbp"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/Sajjad_cloned_ready\n",
        "!cp Concat_quad.pkl /content/gdrive/MyDrive/Sajjad_cloned_ready/Concat_quad_scaled.pkl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files \n",
        "files.upload()"
      ],
      "metadata": {
        "id": "tsWm80vqOIej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is when you ony wat to read the already dataset"
      ],
      "metadata": {
        "id": "fpdWo-_txekD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZrRi3kNhaCf"
      },
      "outputs": [],
      "source": [
        "Concat_quad_pkl = pd.read_pickle('Concat_quad.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZquWzJCd_pR-"
      },
      "source": [
        "Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHfFus3PB3Wg"
      },
      "outputs": [],
      "source": [
        "# # In the charge column some of the rows' value is: '5 puis 10' which is not a float so we repalced it by the 10.\n",
        "# Concat_quad_pkl = Concat_quad.replace('5 puis 10',10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J5lydHzap9A"
      },
      "outputs": [],
      "source": [
        "# scale = MinMaxScaler()\n",
        "# Concat_quad_pkl[['EEG0','EEG1','EEG2','EEG3','% Charge']] = scale.fit_transform(Concat_quad_pkl[['EEG0','EEG1','EEG2','EEG3','% Charge']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xkGlUOME4W_"
      },
      "outputs": [],
      "source": [
        "# #Saving data to gdrive\n",
        "# pd.to_pickle(Concat_quad_pkl, 'Concat_quad_pkl_scaled.pkl')\n",
        "# !cp 'Concat_quad_pkl_scaled.pkl' /content/gdrive/MyDrive/Sajjad_cloned_ready/Concat_quad_scaled.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading preprocessed pkl "
      ],
      "metadata": {
        "id": "vY1iq0V-ldrc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPAkWL97p3h-"
      },
      "outputs": [],
      "source": [
        "Concat_quad_pkl = pd.read_pickle('/content/gdrive/MyDrive/Sajjad_cloned_ready/Concat_quad_scaled.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml19BZdg_SiR"
      },
      "source": [
        "Label encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCcMqJl-UBrV"
      },
      "source": [
        "Dataset and label prepration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txH2zikHfJ5i"
      },
      "outputs": [],
      "source": [
        "Concat_quad_pkl = Concat_quad_pkl.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_9qcYULF1T9"
      },
      "outputs": [],
      "source": [
        "# Choosing only the good EEGs\n",
        "Dataset = Concat_quad_pkl[(Concat_quad_pkl['Qualité clinique']== '+++') | (Concat_quad_pkl['Qualité clinique']== '++')][['EEG0','EEG1','EEG2','EEG3','% Charge']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp2YXFKxHptr"
      },
      "outputs": [],
      "source": [
        "# We defined a function to make a dataset with demanded element size\n",
        "\n",
        "def elementor(df, element_size):\n",
        "  t0 = df[list(df.columns)[0:-1]].values\n",
        "  round = int(t0.shape[0]/element_size)\n",
        "  t0 = t0[0:(round*element_size)]\n",
        "  return t0.reshape((round, element_size, df.shape[1]-1)), np.array([df[list(df.columns)[-1]].iloc[i*element_size:(i+1)*element_size].mean()  for i in range(round)])\n",
        "\n",
        "data, label = elementor(Dataset,196)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRbGb1wqijos"
      },
      "outputs": [],
      "source": [
        "TrainsetF,TestsetF, LabelFairl, TestLabelFairl=train_test_split(data ,label,test_size=0.33)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TrainsetF.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCtEYv4sOq1T",
        "outputId": "216c724c-5d6c-48c1-c782-f6fbf060cd69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33146, 196, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TrainsetF1=TrainsetF.reshape((33146*196*4))"
      ],
      "metadata": {
        "id": "AwHKm7LYPUhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label(x):\n",
        "  return int((x*10))\n",
        "Vlabel=np.vectorize(label) \n",
        "Y=Vlabel(TrainsetF1)"
      ],
      "metadata": {
        "id": "xsouYSU-QZJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import*\n",
        "from keras.layers import*\n",
        "embedding_size = 3\n",
        "model = models.Sequential()\n",
        "model.add(Embedding(input_dim = TrainsetF.shape[0], output_dim = embedding_size, input_length = 1, name=\"embedding\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation=\"relu\"))\n",
        "model.add(Dense(257, activation=\"relu\"))\n",
        "model.add(Dense(10))\n",
        "model.compile(loss = \"mse\", optimizer = \"adam\")\n",
        "model.fit(x = TrainsetF1, y=Y , epochs = 1, batch_size = 4)"
      ],
      "metadata": {
        "id": "iQ0T2uUJO2HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdTXjqh249Vv"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBds2N-tNxNX"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2021 Google Research and The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" PyTorch BigBird model. \"\"\"\n",
        "\n",
        "\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from transformers import Transformer_GPT\n",
        "\n",
        "\n",
        "BIG_BIRD_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"google/bigbird-roberta-base\",\n",
        "    \"google/bigbird-roberta-large\",\n",
        "    \"google/bigbird-base-trivia-itc\",\n",
        "    # See all BigBird models at https://huggingface.co/models?filter=big_bird\n",
        "]\n",
        "\n",
        "_TRIVIA_QA_MAPPING = {\n",
        "    \"big_bird_attention\": \"attention/self\",\n",
        "    \"output_layer_norm\": \"output/LayerNorm\",\n",
        "    \"attention_output\": \"attention/output/dense\",\n",
        "    \"output\": \"output/dense\",\n",
        "    \"self_attention_layer_norm\": \"attention/output/LayerNorm\",\n",
        "    \"intermediate\": \"intermediate/dense\",\n",
        "    \"word_embeddings\": \"bert/embeddings/word_embeddings\",\n",
        "    \"position_embedding\": \"bert/embeddings/position_embeddings\",\n",
        "    \"type_embeddings\": \"bert/embeddings/token_type_embeddings\",\n",
        "    \"embeddings\": \"bert/embeddings\",\n",
        "    \"layer_normalization\": \"output/LayerNorm\",\n",
        "    \"layer_norm\": \"LayerNorm\",\n",
        "    \"trivia_qa_head\": \"qa_classifier\",\n",
        "    \"dense\": \"intermediate/dense\",\n",
        "    \"dense_1\": \"qa_outputs\",\n",
        "}\n",
        "\n",
        "\n",
        "config='big_bird'\n",
        "def load_tf_weights_in_big_bird(model, tf_checkpoint_path, is_trivia_qa=False):\n",
        "    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n",
        "\n",
        "    def load_tf_weights_bert(init_vars, tf_path):\n",
        "        names = []\n",
        "        tf_weights = {}\n",
        "\n",
        "        for name, shape in init_vars:\n",
        "            array = tf.train.load_variable(tf_path, name)\n",
        "            name = name.replace(\"bert/encoder/LayerNorm\", \"bert/embeddings/LayerNorm\")\n",
        "            logger.info(f\"Loading TF weight {name} with shape {shape}\")\n",
        "            names.append(name)\n",
        "            tf_weights[name] = array\n",
        "\n",
        "        return names, tf_weights\n",
        "\n",
        "    def load_tf_weights_trivia_qa(init_vars):\n",
        "        names = []\n",
        "        tf_weights = {}\n",
        "\n",
        "        for i, var in enumerate(init_vars):\n",
        "            name_items = var.name.split(\"/\")\n",
        "\n",
        "            if \"transformer_scaffold\" in name_items[0]:\n",
        "                layer_name_items = name_items[0].split(\"_\")\n",
        "                if len(layer_name_items) < 3:\n",
        "                    layer_name_items += [0]\n",
        "\n",
        "                name_items[0] = f\"bert/encoder/layer_{layer_name_items[2]}\"\n",
        "\n",
        "            name = \"/\".join([_TRIVIA_QA_MAPPING[x] if x in _TRIVIA_QA_MAPPING else x for x in name_items])[\n",
        "                :-2\n",
        "            ]  # remove last :0 in variable\n",
        "\n",
        "            if \"self/attention/output\" in name:\n",
        "                name = name.replace(\"self/attention/output\", \"output\")\n",
        "\n",
        "            if i >= len(init_vars) - 2:\n",
        "                name = name.replace(\"intermediate\", \"output\")\n",
        "\n",
        "            logger.info(f\"Loading TF weight {name} with shape {var.shape}\")\n",
        "            array = var.value().numpy()\n",
        "            names.append(name)\n",
        "            tf_weights[name] = array\n",
        "\n",
        "        return names, tf_weights\n",
        "\n",
        "    try:\n",
        "        import re\n",
        "\n",
        "        import numpy as np\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        logger.error(\n",
        "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
        "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
        "        )\n",
        "        raise\n",
        "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
        "    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n",
        "\n",
        "    # Load weights from TF model\n",
        "    init_vars = tf.saved_model.load(tf_path).variables if is_trivia_qa else tf.train.list_variables(tf_path)\n",
        "\n",
        "    assert len(init_vars) > 0, \"Loaded trained variables cannot be empty.\"\n",
        "\n",
        "    pt_names = list(model.state_dict().keys())\n",
        "\n",
        "    if is_trivia_qa:\n",
        "        names, tf_weights = load_tf_weights_trivia_qa(init_vars)\n",
        "    else:\n",
        "        names, tf_weights = load_tf_weights_bert(init_vars, tf_path)\n",
        "\n",
        "    for txt_name in names:\n",
        "        array = tf_weights[txt_name]\n",
        "        name = txt_name.split(\"/\")\n",
        "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
        "        # which are not required for using pretrained model\n",
        "        if any(\n",
        "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
        "            for n in name\n",
        "        ):\n",
        "            logger.info(f\"Skipping {'/'.join(name)}\")\n",
        "            continue\n",
        "        pointer = model\n",
        "        pt_name = []\n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
        "                scope_names = re.split(r\"_(\\d+)\", m_name)\n",
        "            else:\n",
        "                scope_names = [m_name]\n",
        "            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "                pt_name.append(\"weight\")\n",
        "            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n",
        "                pointer = getattr(pointer, \"bias\")\n",
        "                pt_name.append(\"bias\")\n",
        "            elif scope_names[0] == \"output_weights\":\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "                pt_name.append(\"weight\")\n",
        "            elif scope_names[0] == \"squad\":\n",
        "                pointer = getattr(pointer, \"classifier\")\n",
        "                pt_name.append(\"classifier\")\n",
        "            elif scope_names[0] == \"transform\":\n",
        "                pointer = getattr(pointer, \"transform\")\n",
        "                pt_name.append(\"transform\")\n",
        "                if (\"bias\" in name) or (\"kernel\" in name):\n",
        "                    pointer = getattr(pointer, \"dense\")\n",
        "                    pt_name.append(\"dense\")\n",
        "                elif (\"beta\" in name) or (\"gamma\" in name):\n",
        "                    pointer = getattr(pointer, \"LayerNorm\")\n",
        "                    pt_name.append(\"LayerNorm\")\n",
        "            else:\n",
        "                try:\n",
        "                    pointer = getattr(pointer, scope_names[0])\n",
        "                    pt_name.append(f\"{scope_names[0]}\")\n",
        "                except AttributeError:\n",
        "                    logger.info(f\"Skipping {m_name}\")\n",
        "                    continue\n",
        "            if len(scope_names) >= 2:\n",
        "                num = int(scope_names[1])\n",
        "                pointer = pointer[num]\n",
        "                pt_name.append(f\"{num}\")\n",
        "        if m_name[-11:] == \"_embeddings\" or m_name == \"embeddings\":\n",
        "            pointer = getattr(pointer, \"weight\")\n",
        "            pt_name.append(\"weight\")\n",
        "        elif m_name == \"kernel\":\n",
        "            array = np.transpose(array)\n",
        "        try:\n",
        "            if len(array.shape) > len(pointer.shape) and math.prod(array.shape) == math.prod(pointer.shape):\n",
        "                # print(txt_name, array.shape)\n",
        "                if (\n",
        "                    txt_name.endswith(\"attention/self/key/kernel\")\n",
        "                    or txt_name.endswith(\"attention/self/query/kernel\")\n",
        "                    or txt_name.endswith(\"attention/self/value/kernel\")\n",
        "                ):\n",
        "                    array = array.transpose(1, 0, 2).reshape(pointer.shape)\n",
        "                elif txt_name.endswith(\"attention/output/dense/kernel\"):\n",
        "                    array = array.transpose(0, 2, 1).reshape(pointer.shape)\n",
        "                else:\n",
        "                    array = array.reshape(pointer.shape)\n",
        "\n",
        "            if pointer.shape != array.shape:\n",
        "                raise ValueError(\n",
        "                    f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched of {txt_name}.\"\n",
        "                )\n",
        "        except AssertionError as e:\n",
        "            e.args += (pointer.shape, array.shape)\n",
        "            raise\n",
        "        pt_weight_name = \".\".join(pt_name)\n",
        "        logger.info(f\"Initialize PyTorch weight {pt_weight_name} from {txt_name}.\")\n",
        "        pointer.data = torch.from_numpy(array)\n",
        "        tf_weights.pop(txt_name, None)\n",
        "        pt_names.remove(pt_weight_name)\n",
        "\n",
        "    logger.info(f\"Weights not copied to PyTorch model: {', '.join(tf_weights.keys())}.\")\n",
        "    logger.info(f\"Weights not initialized in PyTorch model: {', '.join(pt_names)}.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "class BigBirdEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        # End copy\n",
        "\n",
        "        self.rescale_embeddings = config.rescale_embeddings\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "\n",
        "        if self.rescale_embeddings:\n",
        "            inputs_embeds = inputs_embeds * (self.hidden_size ** 0.5)\n",
        "\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        embeddings += position_embeddings\n",
        "\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BigBirdSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BigBirdModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BigBirdBlockSparseAttention(nn.Module):\n",
        "    def __init__(self, config, seed=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_seqlen = config.max_position_embeddings\n",
        "        self.seed = seed\n",
        "\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"The hidden size {config.hidden_size} is not a multiple of the number of attention \"\n",
        "                f\"heads {config.num_attention_heads}.\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.num_random_blocks = config.num_random_blocks\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        band_mask=None,\n",
        "        from_mask=None,\n",
        "        to_mask=None,\n",
        "        from_blocked_mask=None,\n",
        "        to_blocked_mask=None,\n",
        "        output_attentions=None,\n",
        "    ):\n",
        "        # Currently this `class` can't be used in decoder.\n",
        "\n",
        "        batch_size, seqlen, _ = hidden_states.size()\n",
        "        to_seq_length = from_seq_length = seqlen\n",
        "        from_block_size = to_block_size = self.block_size\n",
        "\n",
        "        assert from_seq_length % from_block_size == 0, \"Query sided sequence length must be multiple of block size\"\n",
        "        assert to_seq_length % to_block_size == 0, \"Key/Value sided sequence length must be multiple of block size\"\n",
        "\n",
        "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        context_layer, attention_probs = self.bigbird_block_sparse_attention(\n",
        "            query_layer,\n",
        "            key_layer,\n",
        "            value_layer,\n",
        "            band_mask,\n",
        "            from_mask,\n",
        "            to_mask,\n",
        "            from_blocked_mask,\n",
        "            to_blocked_mask,\n",
        "            self.num_attention_heads,\n",
        "            self.num_random_blocks,\n",
        "            self.attention_head_size,\n",
        "            from_block_size,\n",
        "            to_block_size,\n",
        "            batch_size,\n",
        "            from_seq_length,\n",
        "            to_seq_length,\n",
        "            seed=self.seed,\n",
        "            plan_from_length=None,\n",
        "            plan_num_rand_blocks=None,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "\n",
        "        context_layer = context_layer.contiguous().view(batch_size, from_seq_length, -1)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "        return outputs\n",
        "\n",
        "    @staticmethod\n",
        "    def torch_bmm_nd(inp_1, inp_2, ndim=None):\n",
        "        \"\"\"Fast nd matrix multiplication\"\"\"\n",
        "        # faster replacement of torch.einsum (\"bhqk,bhkd->bhqd\")\n",
        "        return torch.bmm(inp_1.reshape((-1,) + inp_1.shape[-2:]), inp_2.reshape((-1,) + inp_2.shape[-2:])).view(\n",
        "            inp_1.shape[: ndim - 2] + (inp_1.shape[ndim - 2], inp_2.shape[ndim - 1])\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def torch_bmm_nd_transpose(inp_1, inp_2, ndim=None):\n",
        "        \"\"\"Fast nd matrix multiplication with transpose\"\"\"\n",
        "        # faster replacement of torch.einsum (bhqd,bhkd->bhqk)\n",
        "        return torch.bmm(\n",
        "            inp_1.reshape((-1,) + inp_1.shape[-2:]), inp_2.reshape((-1,) + inp_2.shape[-2:]).transpose(1, 2)\n",
        "        ).view(inp_1.shape[: ndim - 2] + (inp_1.shape[ndim - 2], inp_2.shape[ndim - 2]))\n",
        "\n",
        "    def bigbird_block_sparse_attention(\n",
        "        self,\n",
        "        query_layer,\n",
        "        key_layer,\n",
        "        value_layer,\n",
        "        band_mask,\n",
        "        from_mask,\n",
        "        to_mask,\n",
        "        from_blocked_mask,\n",
        "        to_blocked_mask,\n",
        "        n_heads,\n",
        "        n_rand_blocks,\n",
        "        attention_head_size,\n",
        "        from_block_size,\n",
        "        to_block_size,\n",
        "        batch_size,\n",
        "        from_seq_len,\n",
        "        to_seq_len,\n",
        "        seed,\n",
        "        plan_from_length,\n",
        "        plan_num_rand_blocks,\n",
        "        output_attentions,\n",
        "    ):\n",
        "\n",
        "        # BigBird block-sparse attention as suggested in paper\n",
        "\n",
        "        # ITC:\n",
        "        #     global tokens: 2 x block_size\n",
        "        #     window tokens: 3 x block_size\n",
        "        #     random tokens: num_rand_tokens x block_size\n",
        "\n",
        "        # ETC:\n",
        "        #     global tokens: extra_globals_tokens + 2 x block_size\n",
        "        #     window tokens: 3 x block_size\n",
        "        #     random tokens: num_rand_tokens x block_size\n",
        "\n",
        "        # Note:\n",
        "        #     1) Currently, ETC is not supported.\n",
        "        #     2) Window size is fixed to 3 blocks & it can be changed only by\n",
        "        #     changing `block_size`.\n",
        "        #     3) Number of global blocks are fixed (2 blocks here) & global tokens can be\n",
        "        #     controlled only by `block_size`.\n",
        "\n",
        "        # attention is calculated separately for q[0], q[1], q[2:-2], q[-2], q[-1] in order to use special trick of shifting tokens (for calculating sliding attention)\n",
        "        # hence following code can be divided into 5 parts.\n",
        "\n",
        "        if from_seq_len // from_block_size != to_seq_len // to_block_size:\n",
        "            raise ValueError(\"Error the number of blocks needs to be same!\")\n",
        "\n",
        "        rsqrt_d = 1 / math.sqrt(attention_head_size)\n",
        "        bsz = batch_size\n",
        "\n",
        "        # generate random attention and corresponding masks\n",
        "        np.random.seed(seed)\n",
        "        if from_seq_len in [1024, 3072, 4096]:  # old plans used in paper\n",
        "            rand_attn = [\n",
        "                self._bigbird_block_rand_mask(\n",
        "                    self.max_seqlen, self.max_seqlen, from_block_size, to_block_size, n_rand_blocks, last_idx=1024\n",
        "                )[: (from_seq_len // from_block_size - 2)]\n",
        "                for _ in range(n_heads)\n",
        "            ]\n",
        "        else:\n",
        "            if plan_from_length is None:\n",
        "                plan_from_length, plan_num_rand_blocks = self._get_rand_attn_plan(\n",
        "                    from_seq_len, from_block_size, n_rand_blocks\n",
        "                )\n",
        "\n",
        "            rand_attn = self._bigbird_block_rand_mask_with_head(\n",
        "                from_seq_length=from_seq_len,\n",
        "                to_seq_length=to_seq_len,\n",
        "                from_block_size=from_block_size,\n",
        "                to_block_size=to_block_size,\n",
        "                num_heads=n_heads,\n",
        "                plan_from_length=plan_from_length,\n",
        "                plan_num_rand_blocks=plan_num_rand_blocks,\n",
        "            )\n",
        "\n",
        "        rand_attn = np.stack(rand_attn, axis=0)\n",
        "        rand_attn = torch.tensor(rand_attn, device=query_layer.device, dtype=torch.long)\n",
        "        rand_attn.unsqueeze_(0)\n",
        "        rand_attn = torch.cat([rand_attn for _ in range(batch_size)], dim=0)\n",
        "\n",
        "        rand_mask = self._create_rand_mask_from_inputs(\n",
        "            from_blocked_mask, to_blocked_mask, rand_attn, n_heads, n_rand_blocks, bsz, from_seq_len, from_block_size\n",
        "        )\n",
        "\n",
        "        blocked_query_matrix = query_layer.view(bsz, n_heads, from_seq_len // from_block_size, from_block_size, -1)\n",
        "        blocked_key_matrix = key_layer.view(bsz, n_heads, to_seq_len // to_block_size, to_block_size, -1)\n",
        "        blocked_value_matrix = value_layer.view(bsz, n_heads, to_seq_len // to_block_size, to_block_size, -1)\n",
        "\n",
        "        # preparing block for randn attn\n",
        "        gathered_key = self.torch_gather_b2(blocked_key_matrix, rand_attn)\n",
        "        gathered_key = gathered_key.view(\n",
        "            bsz, n_heads, to_seq_len // to_block_size - 2, n_rand_blocks * to_block_size, -1\n",
        "        )  # [bsz, n_heads, to_seq_len//to_block_size-2, n_rand_blocks, to_block_size, -1]\n",
        "        gathered_value = self.torch_gather_b2(blocked_value_matrix, rand_attn)\n",
        "        gathered_value = gathered_value.view(\n",
        "            bsz, n_heads, to_seq_len // to_block_size - 2, n_rand_blocks * to_block_size, -1\n",
        "        )  # [bsz, n_heads, to_seq_len//to_block_size-2, n_rand_blocks, to_block_size, -1]\n",
        "\n",
        "        # 1st PART\n",
        "        # 1st block (global block) attention scores\n",
        "        # q[0] x (k[0], k[1], k[2], k[3], k[4] .... )\n",
        "\n",
        "        # [bsz, n_heads, from_block_size, -1] x [bsz, n_heads, to_seq_len, -1] ==> [bsz, n_heads, from_block_size, to_seq_len]\n",
        "        first_product = self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, 0], key_layer, ndim=4)\n",
        "\n",
        "        first_product = first_product * rsqrt_d\n",
        "        first_product += (1.0 - to_mask) * -10000.0\n",
        "        first_attn_weights = F.softmax(first_product, dim=-1)  # [bsz, n_heads, from_block_size, to_seq_len]\n",
        "\n",
        "        # [bsz, n_heads, from_block_size, to_seq_len] x [bsz, n_heads, to_seq_len, -1] ==> [bsz, n_heads, from_block_size, -1]\n",
        "        first_context_layer = self.torch_bmm_nd(first_attn_weights, value_layer, ndim=4)\n",
        "        first_context_layer.unsqueeze_(2)\n",
        "\n",
        "        # 2nd PART\n",
        "        # 2nd block attention scores\n",
        "        # q[1] x (sliding_keys, random_keys, global_keys)\n",
        "        # sliding key blocks -> 2nd, 3rd blocks\n",
        "        # global key blocks -> 1st block\n",
        "\n",
        "        second_key_mat = torch.cat(\n",
        "            [\n",
        "                blocked_key_matrix[:, :, 0],\n",
        "                blocked_key_matrix[:, :, 1],\n",
        "                blocked_key_matrix[:, :, 2],\n",
        "                blocked_key_matrix[:, :, -1],\n",
        "                gathered_key[:, :, 0],\n",
        "            ],\n",
        "            dim=2,\n",
        "        )  # [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1]\n",
        "        second_value_mat = torch.cat(\n",
        "            [\n",
        "                blocked_value_matrix[:, :, 0],\n",
        "                blocked_value_matrix[:, :, 1],\n",
        "                blocked_value_matrix[:, :, 2],\n",
        "                blocked_value_matrix[:, :, -1],\n",
        "                gathered_value[:, :, 0],\n",
        "            ],\n",
        "            dim=2,\n",
        "        )  # [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1]\n",
        "\n",
        "        # [bsz, n_heads, from_block_size, -1] x [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1] ==> [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size]\n",
        "        second_product = self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, 1], second_key_mat, ndim=4)\n",
        "        second_seq_pad = torch.cat(\n",
        "            [\n",
        "                to_mask[:, :, :, : 3 * to_block_size],\n",
        "                to_mask[:, :, :, -to_block_size:],\n",
        "                first_context_layer.new_ones([bsz, 1, 1, n_rand_blocks * to_block_size]),\n",
        "            ],\n",
        "            dim=3,\n",
        "        )\n",
        "        second_rand_pad = torch.cat(\n",
        "            [\n",
        "                first_context_layer.new_ones([bsz, n_heads, from_block_size, 4 * to_block_size]),\n",
        "                rand_mask[:, :, 0],\n",
        "            ],\n",
        "            dim=3,\n",
        "        )\n",
        "        second_product = second_product * rsqrt_d\n",
        "        second_product += (1.0 - torch.minimum(second_seq_pad, second_rand_pad)) * -10000.0\n",
        "        second_attn_weights = F.softmax(\n",
        "            second_product, dim=-1\n",
        "        )  # [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size]\n",
        "\n",
        "        # [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size] x [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1] ==> [bsz, n_heads, from_block_size, -1]\n",
        "        second_context_layer = self.torch_bmm_nd(second_attn_weights, second_value_mat, ndim=4)\n",
        "\n",
        "        second_context_layer.unsqueeze_(2)\n",
        "\n",
        "        # 3rd PART\n",
        "        # Middle blocks attention scores\n",
        "        # q[-2:2] x (sliding_keys, random_keys, global_keys)\n",
        "        # sliding attn is calculated using special trick of shifting tokens as discussed in paper\n",
        "        # random keys are generated by taking random indices as per `rand_attn`\n",
        "        # global keys -> 1st & last block\n",
        "\n",
        "        exp_blocked_key_matrix = torch.cat(\n",
        "            [blocked_key_matrix[:, :, 1:-3], blocked_key_matrix[:, :, 2:-2], blocked_key_matrix[:, :, 3:-1]], dim=3\n",
        "        )  # [bsz, n_heads, from_seq_len//from_block_size-4, 3*to_block_size, -1]\n",
        "        exp_blocked_value_matrix = torch.cat(\n",
        "            [blocked_value_matrix[:, :, 1:-3], blocked_value_matrix[:, :, 2:-2], blocked_value_matrix[:, :, 3:-1]],\n",
        "            dim=3,\n",
        "        )  # [bsz, n_heads, from_seq_len//from_block_size-4, 3*to_block_size, -1]\n",
        "        middle_query_matrix = blocked_query_matrix[:, :, 2:-2]\n",
        "\n",
        "        # sliding attention scores for q[-2:2]\n",
        "        # [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1] x [b, n_heads, from_seq_len//from_block_size-4, 3*to_block_size, -1]\n",
        "        inner_band_product = self.torch_bmm_nd_transpose(middle_query_matrix, exp_blocked_key_matrix, ndim=5)\n",
        "        #     ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, 3*to_block_size]\n",
        "        inner_band_product = inner_band_product * rsqrt_d\n",
        "\n",
        "        # randn attention scores for q[-2:2]\n",
        "        # [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1] x [bsz, n_heads, from_seq_len//from_block_size-4, n_rand_blocks*to_block_size, -1]\n",
        "        rand_band_product = self.torch_bmm_nd_transpose(middle_query_matrix, gathered_key[:, :, 1:-1], ndim=5)\n",
        "        #     ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, n_rand_blocks*to_block_size]\n",
        "        rand_band_product = rand_band_product * rsqrt_d\n",
        "\n",
        "        # Including 1st block (since it's global)\n",
        "        first_band_product = torch.einsum(\n",
        "            \"bhlqd,bhkd->bhlqk\", middle_query_matrix, blocked_key_matrix[:, :, 0]\n",
        "        )  # [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1] x [bsz, n_heads, to_block_size, -1] ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, to_block_size]\n",
        "        first_band_product = first_band_product * rsqrt_d\n",
        "\n",
        "        # Including last block (since it's global)\n",
        "        last_band_product = torch.einsum(\n",
        "            \"bhlqd,bhkd->bhlqk\", middle_query_matrix, blocked_key_matrix[:, :, -1]\n",
        "        )  # [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1] x [bsz, n_heads, to_block_size, -1] ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, to_block_size]\n",
        "        last_band_product = last_band_product * rsqrt_d\n",
        "\n",
        "        # masking padded tokens\n",
        "        inner_band_product += (1.0 - band_mask) * -10000.0\n",
        "        first_band_product += (1.0 - to_mask[:, :, :, :to_block_size].unsqueeze(3)) * -10000.0\n",
        "        last_band_product += (1.0 - to_mask[:, :, :, -to_block_size:].unsqueeze(3)) * -10000.0\n",
        "        rand_band_product += (1.0 - rand_mask[:, :, 1:-1]) * -10000.0\n",
        "\n",
        "        # completing attention scores matrix for all q[-2:2]\n",
        "        band_product = torch.cat(\n",
        "            [first_band_product, inner_band_product, rand_band_product, last_band_product], dim=-1\n",
        "        )  # [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, (5+n_rand_blocks)*to_block_size]\n",
        "\n",
        "        # safely doing softmax since attention matrix is completed\n",
        "        attn_weights = F.softmax(\n",
        "            band_product, dim=-1\n",
        "        )  # [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, (5+n_rand_blocks)*to_block_size]\n",
        "\n",
        "        # contribution of sliding keys\n",
        "        # [bsz, n_heads, m//from_block_size-4, from_block_size, 3*to_block_size] x [bsz, n_heads, from_seq_len//from_block_size-4, 3*to_block_size, -1]\n",
        "        context_layer = self.torch_bmm_nd(\n",
        "            attn_weights[:, :, :, :, to_block_size : 4 * to_block_size], exp_blocked_value_matrix, ndim=5\n",
        "        )\n",
        "        #     ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1]\n",
        "\n",
        "        # adding contribution of random keys\n",
        "        # [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, n_rand_blocks*to_block_size] x [bsz, n_heads, from_seq_len//from_block_size-4, n_rand_blocks*to_block_size, -1]\n",
        "        context_layer += self.torch_bmm_nd(\n",
        "            attn_weights[:, :, :, :, 4 * to_block_size : -to_block_size], gathered_value[:, :, 1:-1], ndim=5\n",
        "        )\n",
        "        #     ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1]\n",
        "\n",
        "        # adding contribution of global keys\n",
        "        context_layer += torch.einsum(\n",
        "            \"bhlqk,bhkd->bhlqd\", attn_weights[:, :, :, :, :to_block_size], blocked_value_matrix[:, :, 0]\n",
        "        )  # [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, to_block_size] x [bsz, n_heads, to_block_size, -1] ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1]\n",
        "        context_layer += torch.einsum(\n",
        "            \"bhlqk,bhkd->bhlqd\", attn_weights[:, :, :, :, -to_block_size:], blocked_value_matrix[:, :, -1]\n",
        "        )  # [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, to_block_size] x [bsz, n_heads, to_block_size, -1] ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1]\n",
        "\n",
        "        # 4th PART\n",
        "        # last 2nd token attention scores\n",
        "        # q[-2] x (sliding_keys, random_keys, global_keys)\n",
        "        # sliding key blocks -> last 3 blocks\n",
        "        # global key block -> 1st block\n",
        "        # random key block -> based on indices stored in `randn_attn`\n",
        "\n",
        "        second_last_key_mat = torch.cat(\n",
        "            [\n",
        "                blocked_key_matrix[:, :, 0],\n",
        "                blocked_key_matrix[:, :, -3],\n",
        "                blocked_key_matrix[:, :, -2],\n",
        "                blocked_key_matrix[:, :, -1],\n",
        "                gathered_key[:, :, -1],\n",
        "            ],\n",
        "            dim=2,\n",
        "        )  # [bsz, n_heads, (4+n_random_blocks)*to_block_size, -1]\n",
        "        second_last_value_mat = torch.cat(\n",
        "            [\n",
        "                blocked_value_matrix[:, :, 0],\n",
        "                blocked_value_matrix[:, :, -3],\n",
        "                blocked_value_matrix[:, :, -2],\n",
        "                blocked_value_matrix[:, :, -1],\n",
        "                gathered_value[:, :, -1],\n",
        "            ],\n",
        "            dim=2,\n",
        "        )  # [bsz, n_heads, (4+r)*to_block_size, -1]\n",
        "\n",
        "        # [bsz, n_heads, from_block_size, -1] x [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1] ==> [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size]\n",
        "        second_last_product = self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, -2], second_last_key_mat, ndim=4)\n",
        "        second_last_seq_pad = torch.cat(\n",
        "            [\n",
        "                to_mask[:, :, :, :to_block_size],\n",
        "                to_mask[:, :, :, -3 * to_block_size :],\n",
        "                context_layer.new_ones([bsz, 1, 1, n_rand_blocks * to_block_size]),\n",
        "            ],\n",
        "            dim=3,\n",
        "        )\n",
        "        second_last_rand_pad = torch.cat(\n",
        "            [\n",
        "                context_layer.new_ones([bsz, n_heads, from_block_size, 4 * to_block_size]),\n",
        "                rand_mask[:, :, -1],\n",
        "            ],\n",
        "            dim=3,\n",
        "        )\n",
        "        second_last_product = second_last_product * rsqrt_d\n",
        "        second_last_product += (1.0 - torch.minimum(second_last_seq_pad, second_last_rand_pad)) * -10000.0\n",
        "        second_last_attn_weights = F.softmax(\n",
        "            second_last_product, dim=-1\n",
        "        )  # [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size]\n",
        "\n",
        "        # [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size] x [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1] ==> [bsz, n_heads, from_block_size, -1]\n",
        "        second_last_context_layer = self.torch_bmm_nd(second_last_attn_weights, second_last_value_mat, ndim=4)\n",
        "        second_last_context_layer.unsqueeze_(2)\n",
        "\n",
        "        # 5th PART\n",
        "        # last block (global) attention scores\n",
        "        # q[-1] x (k[0], k[1], k[2], k[3], .... )\n",
        "\n",
        "        # [bsz, n_heads, from_block_size, -1] x [bsz, n_heads, to_seq_len, -1] ==> [bsz, n_heads, from_block_size, to_seq_len]\n",
        "        last_product = self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, -1], key_layer, ndim=4)\n",
        "        last_product = last_product * rsqrt_d\n",
        "        last_product += (1.0 - to_mask) * -10000.0\n",
        "        last_attn_weights = F.softmax(last_product, dim=-1)  # [bsz, n_heads, from_block_size, n]\n",
        "\n",
        "        # [bsz, n_heads, from_block_size, to_seq_len] x [bsz, n_heads, to_seq_len, -1] ==> [bsz, n_heads, from_block_size, -1]\n",
        "        last_context_layer = self.torch_bmm_nd(last_attn_weights, value_layer, ndim=4)\n",
        "        last_context_layer.unsqueeze_(2)\n",
        "\n",
        "        # combining representations of all tokens\n",
        "        context_layer = torch.cat(\n",
        "            [first_context_layer, second_context_layer, context_layer, second_last_context_layer, last_context_layer],\n",
        "            dim=2,\n",
        "        )\n",
        "        context_layer = context_layer.view((bsz, n_heads, from_seq_len, -1)) * from_mask\n",
        "        context_layer = torch.transpose(context_layer, 1, 2)\n",
        "\n",
        "        # this is just for visualizing; forward pass doesn't depend on following code\n",
        "        if output_attentions:\n",
        "            # TODO(PVP): need to verify if below code is correct\n",
        "            attention_probs = torch.zeros(\n",
        "                bsz, n_heads, from_seq_len, to_seq_len, dtype=torch.float, device=context_layer.device\n",
        "            )\n",
        "\n",
        "            # 1st query block\n",
        "            # corresponding to `first_context_layer`\n",
        "            attention_probs[:, :, :from_block_size, :] = first_attn_weights  # all keys global\n",
        "\n",
        "            # 2nd query block\n",
        "            # corresponding to `second_context_layer`\n",
        "            attention_probs[:, :, from_block_size : 2 * from_block_size, : 3 * to_block_size] = second_attn_weights[\n",
        "                :, :, :, : 3 * to_block_size\n",
        "            ]  # 1st three key blocks (global + sliding)\n",
        "            attention_probs[:, :, from_block_size : 2 * from_block_size, -to_block_size:] = second_attn_weights[\n",
        "                :, :, :, 3 * to_block_size : 4 * to_block_size\n",
        "            ]  # last key block (global)\n",
        "            # random keys\n",
        "            for p1, i1, w1 in zip(range(bsz), rand_attn, second_attn_weights):\n",
        "                # p1, i1, w1 corresponds to batch_dim i.e. following operation is done for each sequence in batch\n",
        "                for p2, i2, w2 in zip(range(n_heads), i1, w1):\n",
        "                    # p2, i2, w2 corresponds to head_dim i.e. following operation is done for each heads\n",
        "                    attn_probs_view = attention_probs.view(\n",
        "                        bsz,\n",
        "                        n_heads,\n",
        "                        from_seq_len // from_block_size,\n",
        "                        from_block_size,\n",
        "                        to_seq_len // to_block_size,\n",
        "                        to_block_size,\n",
        "                    )\n",
        "                    right_slice = w2[:, 4 * to_block_size :]\n",
        "                    attn_probs_view[p1, p2, 1, :, i2[0]] = right_slice.view(\n",
        "                        from_block_size, n_rand_blocks, to_block_size\n",
        "                    )\n",
        "\n",
        "            # Middle query blocks\n",
        "            # corresponding to `context_layer`\n",
        "            # sliding keys\n",
        "            for q_idx in range(from_seq_len // from_block_size - 4):\n",
        "                attn_probs_view = attention_probs.view(\n",
        "                    bsz,\n",
        "                    n_heads,\n",
        "                    from_seq_len // from_block_size,\n",
        "                    from_block_size,\n",
        "                    to_seq_len // to_block_size,\n",
        "                    to_block_size,\n",
        "                )[:, :, 2:-2, :, 1:-1, :]\n",
        "                right_slice = attn_weights[:, :, q_idx, :, to_block_size : 4 * to_block_size]\n",
        "                attn_probs_view[:, :, q_idx, :, q_idx : q_idx + 3, :] = right_slice.view(\n",
        "                    bsz, n_heads, from_block_size, 3, to_block_size\n",
        "                )  # inner_band_product\n",
        "            # global keys (corresponding to 1st key block)\n",
        "            attention_probs[:, :, 2 * from_block_size : -2 * from_block_size, :to_block_size] = attn_weights[\n",
        "                :, :, :, :, :to_block_size\n",
        "            ].view(\n",
        "                bsz, n_heads, -1, to_block_size\n",
        "            )  # first_band_product\n",
        "            # global keys (corresponding to last key block)\n",
        "            attention_probs[:, :, 2 * from_block_size : -2 * from_block_size, -to_block_size:] = attn_weights[\n",
        "                :, :, :, :, -to_block_size:\n",
        "            ].view(\n",
        "                bsz, n_heads, -1, to_block_size\n",
        "            )  # last_band_product\n",
        "            # random keys\n",
        "            for p1, i1, w1 in zip(range(bsz), rand_attn, attn_weights):\n",
        "                # p1, i1, w1 corresponds to batch_dim i.e. following operation is done for each sequence in batch\n",
        "                for p2, i2, w2 in zip(range(n_heads), i1, w1):\n",
        "                    # p2, i2, w2 corresponds to head_dim i.e. following operation is done for each heads\n",
        "                    for q_idx in range(1, len(i2) - 1):\n",
        "                        attn_probs_view = attention_probs.view(\n",
        "                            bsz,\n",
        "                            n_heads,\n",
        "                            from_seq_len // from_block_size,\n",
        "                            from_block_size,\n",
        "                            to_seq_len // to_block_size,\n",
        "                            to_block_size,\n",
        "                        )\n",
        "                        right_slice = w2[q_idx - 1, :, 4 * to_block_size : -to_block_size]\n",
        "                        attn_probs_view[p1, p2, q_idx + 1, :, i2[q_idx]] = right_slice.view(\n",
        "                            from_block_size, n_rand_blocks, to_block_size\n",
        "                        )\n",
        "\n",
        "            # Second-last query block\n",
        "            # corresponding to `second_last_context_layer`\n",
        "            attention_probs[:, :, -2 * from_block_size : -from_block_size, :to_block_size] = second_last_attn_weights[\n",
        "                :, :, :, :to_block_size\n",
        "            ]  # 1st key block (global)\n",
        "            attention_probs[\n",
        "                :, :, -2 * from_block_size : -from_block_size, -3 * to_block_size :\n",
        "            ] = second_last_attn_weights[\n",
        "                :, :, :, to_block_size : 4 * to_block_size\n",
        "            ]  # last three blocks (global + sliding)\n",
        "            # random keys\n",
        "            for p1, i1, w1 in zip(range(bsz), rand_attn, second_last_attn_weights):\n",
        "                # p1, i1, w1 corresponds to batch_dim i.e. following operation is done for each sequence in batch\n",
        "                for p2, i2, w2 in zip(range(n_heads), i1, w1):\n",
        "                    # p2, i2, w2 corresponds to head_dim i.e. following operation is done for each heads\n",
        "                    attn_probs_view = attention_probs.view(\n",
        "                        bsz,\n",
        "                        n_heads,\n",
        "                        from_seq_len // from_block_size,\n",
        "                        from_block_size,\n",
        "                        to_seq_len // to_block_size,\n",
        "                        to_block_size,\n",
        "                    )\n",
        "                    right_slice = w2[:, 4 * to_block_size :]\n",
        "                    attn_probs_view[p1, p2, -2, :, i2[-1]] = right_slice.view(\n",
        "                        from_block_size, n_rand_blocks, to_block_size\n",
        "                    )\n",
        "\n",
        "            # last query block\n",
        "            # corresponding to `last_context_layer`\n",
        "            attention_probs[:, :, -from_block_size:, :] = last_attn_weights  # all keys global\n",
        "\n",
        "        else:\n",
        "            attention_probs = None\n",
        "\n",
        "        return context_layer, attention_probs\n",
        "\n",
        "    @staticmethod\n",
        "    def torch_gather_b2(params, indices):\n",
        "        # this operation is equivalent to tf.gather when batch_dims=2\n",
        "\n",
        "        if params.shape[:2] != indices.shape[:2]:\n",
        "            raise ValueError(\n",
        "                f\"Make sure that the first two dimensions of params and indices are identical, \\\n",
        "                but they are params: {params.shape[:2]} vs. indices: {params.shape[:2]}\"\n",
        "            )\n",
        "        num_indices_to_gather = indices.shape[-2] * indices.shape[-1]\n",
        "        num_indices_to_pick_from = params.shape[2]\n",
        "\n",
        "        indices_shift = (\n",
        "            torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
        "            // num_indices_to_gather\n",
        "            * num_indices_to_pick_from\n",
        "        )\n",
        "\n",
        "        flattened_indices = indices.view(-1) + indices_shift\n",
        "        flattened_params = params.reshape(-1, params.shape[-2], params.shape[-1])\n",
        "\n",
        "        out_flattened = flattened_params.index_select(0, flattened_indices)\n",
        "\n",
        "        out = out_flattened.reshape(params.shape[:2] + (num_indices_to_gather,) + params.shape[3:])\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_rand_mask_from_inputs(\n",
        "        from_blocked_mask,\n",
        "        to_blocked_mask,\n",
        "        rand_attn,\n",
        "        num_attention_heads,\n",
        "        num_rand_blocks,\n",
        "        batch_size,\n",
        "        from_seq_length,\n",
        "        from_block_size,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create 3D attention mask from a 2D tensor mask.\n",
        "\n",
        "        Args:\n",
        "            from_blocked_mask: 2D Tensor of shape [batch_size,\n",
        "            from_seq_length//from_block_size, from_block_size].\n",
        "            to_blocked_mask: int32 Tensor of shape [batch_size,\n",
        "            to_seq_length//to_block_size, to_block_size].\n",
        "            rand_attn: [batch_size, num_attention_heads,\n",
        "            from_seq_length//from_block_size-2, num_rand_blocks]\n",
        "            num_attention_heads: int. Number of attention heads.\n",
        "            num_rand_blocks: int. Number of random chunks per row.\n",
        "            batch_size: int. Batch size for computation.\n",
        "            from_seq_length: int. length of from sequence.\n",
        "            from_block_size: int. size of block in from sequence.\n",
        "\n",
        "        Returns:\n",
        "            float Tensor of shape [batch_size, num_attention_heads, from_seq_length//from_block_size-2,\n",
        "            from_block_size, num_rand_blocks*to_block_size].\n",
        "        \"\"\"\n",
        "        num_windows = from_seq_length // from_block_size - 2\n",
        "        rand_mask = torch.stack([p1[i1.flatten()] for p1, i1 in zip(to_blocked_mask, rand_attn)])\n",
        "        rand_mask = rand_mask.view(batch_size, num_attention_heads, num_windows, num_rand_blocks * from_block_size)\n",
        "        rand_mask = torch.einsum(\"blq,bhlk->bhlqk\", from_blocked_mask[:, 1:-1], rand_mask)\n",
        "        return rand_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_rand_attn_plan(from_seq_length, from_block_size, num_rand_blocks):\n",
        "        \"\"\"\n",
        "        Gives the plan of where to put random attention.\n",
        "\n",
        "        Args:\n",
        "            from_seq_length: int. length of from sequence.\n",
        "            from_block_size: int. size of block in from sequence.\n",
        "            num_rand_blocks: int. Number of random chunks per row.\n",
        "\n",
        "        Returns:\n",
        "            plan_from_length: ending location of from block plan_num_rand_blocks: number of random ending location for\n",
        "            each block\n",
        "        \"\"\"\n",
        "\n",
        "        plan_from_length = []\n",
        "        plan_num_rand_blocks = []\n",
        "        if (2 * num_rand_blocks + 5) < (from_seq_length // from_block_size):\n",
        "            plan_from_length.append(int((2 * num_rand_blocks + 5) * from_block_size))\n",
        "            plan_num_rand_blocks.append(num_rand_blocks)\n",
        "            plan_from_length.append(from_seq_length)\n",
        "            plan_num_rand_blocks.append(0)\n",
        "        elif (num_rand_blocks + 5) < (from_seq_length // from_block_size):\n",
        "            plan_from_length.append(int((num_rand_blocks + 5) * from_block_size))\n",
        "            plan_num_rand_blocks.append(num_rand_blocks // 2)\n",
        "            plan_from_length.append(from_seq_length)\n",
        "            plan_num_rand_blocks.append(num_rand_blocks - (num_rand_blocks // 2))\n",
        "        else:\n",
        "            plan_from_length.append(from_seq_length)\n",
        "            plan_num_rand_blocks.append(num_rand_blocks)\n",
        "\n",
        "        return plan_from_length, plan_num_rand_blocks\n",
        "\n",
        "    @staticmethod\n",
        "    def _bigbird_block_rand_mask(\n",
        "        from_seq_length, to_seq_length, from_block_size, to_block_size, num_rand_blocks, last_idx=-1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create adjacency list of random attention.\n",
        "\n",
        "        Args:\n",
        "            from_seq_length: int. length of from sequence.\n",
        "            to_seq_length: int. length of to sequence.\n",
        "            from_block_size: int. size of block in from sequence.\n",
        "            to_block_size: int. size of block in to sequence.\n",
        "            num_rand_blocks: int. Number of random chunks per row.\n",
        "            last_idx: if -1 then num_rand_blocks blocks chosen anywhere in to sequence,\n",
        "            if positive then num_rand_blocks blocks chosen only up to last_idx.\n",
        "\n",
        "        Returns:\n",
        "            adjacency list of size from_seq_length//from_block_size-2 by num_rand_blocks\n",
        "        \"\"\"\n",
        "        # using this method when from_seq_length in [1024, 3072, 4096]\n",
        "\n",
        "        assert (\n",
        "            from_seq_length // from_block_size == to_seq_length // to_block_size\n",
        "        ), \"Error the number of blocks needs to be same!\"\n",
        "\n",
        "        rand_attn = np.zeros((from_seq_length // from_block_size - 2, num_rand_blocks), dtype=np.int32)\n",
        "        middle_seq = np.arange(1, to_seq_length // to_block_size - 1, dtype=np.int32)\n",
        "        last = to_seq_length // to_block_size - 1\n",
        "        if last_idx > (2 * to_block_size):\n",
        "            last = (last_idx // to_block_size) - 1\n",
        "\n",
        "        r = num_rand_blocks  # shorthand\n",
        "        for i in range(1, from_seq_length // from_block_size - 1):\n",
        "            start = i - 2\n",
        "            end = i\n",
        "            if i == 1:\n",
        "                rand_attn[i - 1, :] = np.random.permutation(middle_seq[2:last])[:r]\n",
        "            elif i == 2:\n",
        "                rand_attn[i - 1, :] = np.random.permutation(middle_seq[3:last])[:r]\n",
        "            elif i == from_seq_length // from_block_size - 3:\n",
        "                rand_attn[i - 1, :] = np.random.permutation(middle_seq[:last])[:r]\n",
        "            # Missing -3: should have been sliced till last-3\n",
        "            elif i == from_seq_length // from_block_size - 2:\n",
        "                rand_attn[i - 1, :] = np.random.permutation(middle_seq[:last])[:r]\n",
        "            # Missing -4: should have been sliced till last-4\n",
        "            else:\n",
        "                if start > last:\n",
        "                    start = last\n",
        "                    rand_attn[i - 1, :] = np.random.permutation(middle_seq[:start])[:r]\n",
        "                elif (end + 1) == last:\n",
        "                    rand_attn[i - 1, :] = np.random.permutation(middle_seq[:start])[:r]\n",
        "                else:\n",
        "                    rand_attn[i - 1, :] = np.random.permutation(\n",
        "                        np.concatenate((middle_seq[:start], middle_seq[end + 1 : last]))\n",
        "                    )[:r]\n",
        "        return rand_attn\n",
        "\n",
        "    def _bigbird_block_rand_mask_with_head(\n",
        "        self,\n",
        "        from_seq_length,\n",
        "        to_seq_length,\n",
        "        from_block_size,\n",
        "        to_block_size,\n",
        "        num_heads,\n",
        "        plan_from_length,\n",
        "        plan_num_rand_blocks,\n",
        "        window_block_left=1,\n",
        "        window_block_right=1,\n",
        "        global_block_top=1,\n",
        "        global_block_bottom=1,\n",
        "        global_block_left=1,\n",
        "        global_block_right=1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create adjacency list of random attention.\n",
        "\n",
        "        Args:\n",
        "            from_seq_length: int. length of from sequence.\n",
        "            to_seq_length: int. length of to sequence.\n",
        "            from_block_size: int. size of block in from sequence.\n",
        "            to_block_size: int. size of block in to sequence.\n",
        "            num_heads: int. total number of heads.\n",
        "            plan_from_length: list. plan from length where num_random_blocks are choosen from.\n",
        "            plan_num_rand_blocks: list. number of rand blocks within the plan.\n",
        "            window_block_left: int. number of blocks of window to left of a block.\n",
        "            window_block_right: int. number of blocks of window to right of a block.\n",
        "            global_block_top: int. number of blocks at the top.\n",
        "            global_block_bottom: int. number of blocks at the bottom.\n",
        "            global_block_left: int. Number of blocks globally used to the left.\n",
        "            global_block_right: int. Number of blocks globally used to the right.\n",
        "\n",
        "        Returns:\n",
        "            adjacency list of size num_head where each element is of size from_seq_length//from_block_size-2 by\n",
        "            num_rand_blocks\n",
        "        \"\"\"\n",
        "        # using this method when from_seq_length not in [1024, 3072, 4096]\n",
        "\n",
        "        assert (\n",
        "            from_seq_length // from_block_size == to_seq_length // to_block_size\n",
        "        ), \"Error the number of blocks needs to be same!\"\n",
        "\n",
        "        assert from_seq_length in plan_from_length, \"Error from sequence length not in plan!\"\n",
        "\n",
        "        # Total number of blocks in the mmask\n",
        "        num_blocks = from_seq_length // from_block_size\n",
        "        # Number of blocks per plan\n",
        "        plan_block_length = np.array(plan_from_length) // from_block_size\n",
        "        # till when to follow plan\n",
        "        max_plan_idx = plan_from_length.index(from_seq_length)\n",
        "        # Random Attention adjacency list\n",
        "        rand_attn = [\n",
        "            np.zeros((num_blocks, np.sum(plan_num_rand_blocks[: max_plan_idx + 1])), dtype=np.int32)\n",
        "            for i in range(num_heads)\n",
        "        ]\n",
        "\n",
        "        # We will go iteratively over the plan blocks and pick random number of\n",
        "        # Attention blocks from the legally allowed blocks\n",
        "        for plan_idx in range(max_plan_idx + 1):\n",
        "            rnd_r_cnt = 0\n",
        "            if plan_idx > 0:\n",
        "                # set the row for all from_blocks starting from 0 to\n",
        "                # plan_block_length[plan_idx-1]\n",
        "                # column indx start fromm plan_block_length[plan_idx-1] and ends at\n",
        "                # plan_block_length[plan_idx]\n",
        "                if plan_num_rand_blocks[plan_idx] > 0:\n",
        "                    rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx]))\n",
        "                    curr_r_cnt = int(np.sum(plan_num_rand_blocks[: plan_idx + 1]))\n",
        "                    for blk_rw_idx in range(global_block_top, plan_block_length[plan_idx - 1]):\n",
        "                        for h in range(num_heads):\n",
        "                            rand_attn[h][blk_rw_idx, rnd_r_cnt:curr_r_cnt] = self._get_single_block_row_attention(\n",
        "                                block_id=blk_rw_idx,\n",
        "                                to_start_block_id=plan_block_length[plan_idx - 1],\n",
        "                                to_end_block_id=plan_block_length[plan_idx],\n",
        "                                num_rand_blocks=plan_num_rand_blocks[plan_idx],\n",
        "                                window_block_left=window_block_left,\n",
        "                                window_block_right=window_block_right,\n",
        "                                global_block_left=global_block_left,\n",
        "                                global_block_right=global_block_right,\n",
        "                            )\n",
        "\n",
        "                for pl_id in range(plan_idx):\n",
        "                    if plan_num_rand_blocks[pl_id] == 0:\n",
        "                        continue\n",
        "                    for blk_rw_idx in range(plan_block_length[plan_idx - 1], plan_block_length[plan_idx]):\n",
        "                        rnd_r_cnt = 0\n",
        "                        to_start_block_id = 0\n",
        "                        if pl_id > 0:\n",
        "                            rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:pl_id]))\n",
        "                            to_start_block_id = plan_block_length[pl_id - 1]\n",
        "                        curr_r_cnt = int(np.sum(plan_num_rand_blocks[: pl_id + 1]))\n",
        "                        for h in range(num_heads):\n",
        "                            rand_attn[h][blk_rw_idx, rnd_r_cnt:curr_r_cnt] = self._get_single_block_row_attention(\n",
        "                                block_id=blk_rw_idx,\n",
        "                                to_start_block_id=to_start_block_id,\n",
        "                                to_end_block_id=plan_block_length[pl_id],\n",
        "                                num_rand_blocks=plan_num_rand_blocks[pl_id],\n",
        "                                window_block_left=window_block_left,\n",
        "                                window_block_right=window_block_right,\n",
        "                                global_block_left=global_block_left,\n",
        "                                global_block_right=global_block_right,\n",
        "                            )\n",
        "\n",
        "            if plan_num_rand_blocks[plan_idx] == 0:\n",
        "                continue\n",
        "            curr_r_cnt = int(np.sum(plan_num_rand_blocks[: plan_idx + 1]))\n",
        "            from_start_block_id = global_block_top\n",
        "            to_start_block_id = 0\n",
        "            if plan_idx > 0:\n",
        "                rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx]))\n",
        "                from_start_block_id = plan_block_length[plan_idx - 1]\n",
        "                to_start_block_id = plan_block_length[plan_idx - 1]\n",
        "\n",
        "            for blk_rw_idx in range(from_start_block_id, plan_block_length[plan_idx]):\n",
        "                for h in range(num_heads):\n",
        "                    rand_attn[h][blk_rw_idx, rnd_r_cnt:curr_r_cnt] = self._get_single_block_row_attention(\n",
        "                        block_id=blk_rw_idx,\n",
        "                        to_start_block_id=to_start_block_id,\n",
        "                        to_end_block_id=plan_block_length[plan_idx],\n",
        "                        num_rand_blocks=plan_num_rand_blocks[plan_idx],\n",
        "                        window_block_left=window_block_left,\n",
        "                        window_block_right=window_block_right,\n",
        "                        global_block_left=global_block_left,\n",
        "                        global_block_right=global_block_right,\n",
        "                    )\n",
        "\n",
        "        for nh in range(num_heads):\n",
        "            rand_attn[nh] = rand_attn[nh][global_block_top : num_blocks - global_block_bottom, :]\n",
        "\n",
        "        return rand_attn\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_single_block_row_attention(\n",
        "        block_id,\n",
        "        to_start_block_id,\n",
        "        to_end_block_id,\n",
        "        num_rand_blocks,\n",
        "        window_block_left=1,\n",
        "        window_block_right=1,\n",
        "        global_block_left=1,\n",
        "        global_block_right=1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        For a single row block get random row attention.\n",
        "\n",
        "        Args:\n",
        "            block_id: int. block id of row.\n",
        "            to_start_block_id: int. random attention column start id.\n",
        "            to_end_block_id: int. random attention column end id.\n",
        "            num_rand_blocks: int. number of random blocks to be selected.\n",
        "            window_block_left: int. number of blocks of window to left of a block.\n",
        "            window_block_right: int. number of blocks of window to right of a block.\n",
        "            global_block_left: int. Number of blocks globally used to the left.\n",
        "            global_block_right: int. Number of blocks globally used to the right.\n",
        "\n",
        "        Returns:\n",
        "            row containing the random attention vector of size num_rand_blocks.\n",
        "        \"\"\"\n",
        "        # list of to_blocks from which to choose random attention\n",
        "        to_block_list = np.arange(to_start_block_id, to_end_block_id, dtype=np.int32)\n",
        "        # permute the blocks\n",
        "        perm_block = np.random.permutation(to_block_list)\n",
        "\n",
        "        # illegal blocks for the current block id, using window\n",
        "        illegal_blocks = list(range(block_id - window_block_left, block_id + window_block_right + 1))\n",
        "\n",
        "        # Add blocks at the start and at the end\n",
        "        illegal_blocks.extend(list(range(global_block_left)))\n",
        "        illegal_blocks.extend(list(range(to_end_block_id - global_block_right, to_end_block_id)))\n",
        "\n",
        "        # The second from_block cannot choose random attention on second last to_block\n",
        "        if block_id == 1:\n",
        "            illegal_blocks.append(to_end_block_id - 2)\n",
        "\n",
        "        # The second last from_block cannot choose random attention on second to_block\n",
        "        if block_id == to_end_block_id - 2:\n",
        "            illegal_blocks.append(1)\n",
        "\n",
        "        selected_random_blokcs = []\n",
        "\n",
        "        for i in range(to_end_block_id - to_start_block_id):\n",
        "            if perm_block[i] not in illegal_blocks:\n",
        "                selected_random_blokcs.append(perm_block[i])\n",
        "            if len(selected_random_blokcs) == num_rand_blocks:\n",
        "                break\n",
        "        return np.array(selected_random_blokcs, dtype=np.int32)\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->BigBird\n",
        "class BigBirdSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BigBirdAttention(nn.Module):\n",
        "    def __init__(self, config, seed=None):\n",
        "        super().__init__()\n",
        "        self.attention_type = config.attention_type\n",
        "        self.config = config\n",
        "        self.seed = seed\n",
        "\n",
        "        if self.config.attention_type == \"original_full\":\n",
        "            self.self = BigBirdSelfAttention(config)\n",
        "        elif self.config.attention_type == \"block_sparse\":\n",
        "            self.self = BigBirdBlockSparseAttention(config, seed)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"attention_type can either be original_full or block_sparse, but is {self.config.attention_type}\"\n",
        "            )\n",
        "\n",
        "        self.output = BigBirdSelfOutput(config)\n",
        "\n",
        "    def set_attention_type(self, value: str):\n",
        "        if value not in [\"original_full\", \"block_sparse\"]:\n",
        "            raise ValueError(\n",
        "                f\"attention_type can only be set to either 'original_full' or 'block_sparse', but is {value}\"\n",
        "            )\n",
        "        # attention type is already correctly set\n",
        "        if value == self.attention_type:\n",
        "            return\n",
        "\n",
        "        self.attention_type = value\n",
        "        if value == \"original_full\":\n",
        "            # copy all weights to new full attention class\n",
        "            attn_weights = BigBirdSelfAttention(self.config)\n",
        "        else:\n",
        "            # copy all weights to new sparse attention class\n",
        "            attn_weights = BigBirdBlockSparseAttention(self.config, self.seed)\n",
        "\n",
        "        attn_weights.query = self.self.query\n",
        "        attn_weights.value = self.self.value\n",
        "        attn_weights.key = self.self.key\n",
        "        self.self = attn_weights\n",
        "        self.attention_type = value\n",
        "\n",
        "        if not self.training:\n",
        "            self.self.eval()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "        # block_sparse config\n",
        "        band_mask=None,\n",
        "        from_mask=None,\n",
        "        to_mask=None,\n",
        "        from_blocked_mask=None,\n",
        "        to_blocked_mask=None,\n",
        "    ):\n",
        "\n",
        "        if self.attention_type == \"original_full\":\n",
        "            self_outputs = self.self(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "        else:\n",
        "            assert (\n",
        "                encoder_hidden_states is None\n",
        "            ), \"BigBird cannot be used as a decoder when config.attention_type != 'original_full'\"\n",
        "            self_outputs = self.self(\n",
        "                hidden_states, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, output_attentions\n",
        "            )\n",
        "\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->BigBird\n",
        "class BigBirdIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->BigBird\n",
        "class BigBirdOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BigBirdLayer(nn.Module):\n",
        "    def __init__(self, config, seed=None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.attention_type = config.attention_type\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = BigBirdAttention(config, seed=seed)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
        "            self.crossattention = BigBirdAttention(config)\n",
        "        self.intermediate = BigBirdIntermediate(config)\n",
        "        self.output = BigBirdOutput(config)\n",
        "\n",
        "    def set_attention_type(self, value: str):\n",
        "        if value not in [\"original_full\", \"block_sparse\"]:\n",
        "            raise ValueError(\n",
        "                f\"attention_type can only be set to either 'original_full' or 'block_sparse', but is {value}\"\n",
        "            )\n",
        "        # attention type is already correctly set\n",
        "        if value == self.attention_type:\n",
        "            return\n",
        "        self.attention_type = value\n",
        "        self.attention.set_attention_type(value)\n",
        "\n",
        "        if self.add_cross_attention:\n",
        "            self.crossattention.set_attention_type(value)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        band_mask=None,\n",
        "        from_mask=None,\n",
        "        to_mask=None,\n",
        "        blocked_encoder_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "            band_mask=band_mask,\n",
        "            from_mask=from_mask,\n",
        "            to_mask=to_mask,\n",
        "            from_blocked_mask=blocked_encoder_mask,\n",
        "            to_blocked_mask=blocked_encoder_mask,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            if not hasattr(self, \"crossattention\"):\n",
        "                raise ValueError(\n",
        "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \\\n",
        "                    cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "                )\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "class BigBirdEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.attention_type = config.attention_type\n",
        "\n",
        "        self.layer = nn.ModuleList(\n",
        "            [BigBirdLayer(config, seed=layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
        "        )\n",
        "\n",
        "    def set_attention_type(self, value: str):\n",
        "        if value not in [\"original_full\", \"block_sparse\"]:\n",
        "            raise ValueError(\n",
        "                f\"attention_type can only be set to either 'original_full' or 'block_sparse', but is {value}\"\n",
        "            )\n",
        "        # attention type is already correctly set\n",
        "        if value == self.attention_type:\n",
        "            return\n",
        "        self.attention_type = value\n",
        "        for layer in self.layer:\n",
        "            layer.set_attention_type(value)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        band_mask=None,\n",
        "        from_mask=None,\n",
        "        to_mask=None,\n",
        "        blocked_encoder_mask=None,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warning(\n",
        "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
        "                        \"`use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    band_mask,\n",
        "                    from_mask,\n",
        "                    to_mask,\n",
        "                    blocked_encoder_mask,\n",
        "                )\n",
        "            else:\n",
        "\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    band_mask,\n",
        "                    from_mask,\n",
        "                    to_mask,\n",
        "                    blocked_encoder_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->BigBird\n",
        "class BigBirdPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.transform_act_fn = config.hidden_act\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->BigBird\n",
        "class BigBirdLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.transform = BigBirdPredictionHeadTransform(config)\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->BigBird\n",
        "class BigBirdOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.predictions = BigBirdLMPredictionHead(config)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertOnlyNSPHead with Bert->BigBird\n",
        "class BigBirdOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertPreTrainingHeads with Bert->BigBird\n",
        "class BigBirdPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.predictions = BigBirdLMPredictionHead(config)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QiCM9rB4LerF"
      },
      "outputs": [],
      "source": [
        "A = Transformer_GPT.parameters(TestsetF[15])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "160hLlZhO56N",
        "outputId": "ef99b1d8-0aef-4d59-c030-dfe5275f13b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0         1         2         3         4         5         6    \\\n",
              "0   0.000736  0.003261  0.003091  0.005631  0.005303  0.002512  0.005898   \n",
              "1   0.005558  0.006796  0.001973  0.004064  0.006750  0.003526  0.006443   \n",
              "2   0.002769  0.004815  0.001797  0.003745  0.008263  0.000747  0.003933   \n",
              "3   0.000163  0.006754  0.002327  0.007948  0.004225  0.000736  0.008806   \n",
              "4   0.005102  0.003130  0.005525  0.005910  0.009319  0.005042  0.004585   \n",
              "..       ...       ...       ...       ...       ...       ...       ...   \n",
              "95  0.001138  0.003412  0.000614  0.006940  0.000586  0.000787  0.009720   \n",
              "96  0.001178  0.007707  0.004206  0.000553  0.003721  0.006909  0.009865   \n",
              "97  0.000253  0.000805  0.006576  0.005944  0.005812  0.002645  0.004091   \n",
              "98  0.000795  0.008041  0.007542  0.007155  0.004164  0.004607  0.008145   \n",
              "99  0.007825  0.006430  0.000048  0.007625  0.008290  0.008100  0.009624   \n",
              "\n",
              "         7         8         9    ...       91        92        93        94   \\\n",
              "0   0.000993  0.006528  0.003000  ...  0.002851  0.006006  0.008011  0.008991   \n",
              "1   0.008298  0.000562  0.005633  ...  0.003693  0.000521  0.003585  0.002004   \n",
              "2   0.002002  0.008654  0.007585  ...  0.009344  0.007984  0.007712  0.005985   \n",
              "3   0.009342  0.009554  0.005927  ...  0.006932  0.001151  0.005495  0.009020   \n",
              "4   0.002443  0.003127  0.002256  ...  0.006578  0.009494  0.000140  0.002139   \n",
              "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
              "95  0.003857  0.003234  0.005187  ...  0.003192  0.005011  0.006775  0.007486   \n",
              "96  0.008238  0.009274  0.001312  ...  0.005135  0.001427  0.009901  0.005763   \n",
              "97  0.005904  0.001569  0.004554  ...  0.004521  0.002444  0.004916  0.006115   \n",
              "98  0.004014  0.003970  0.006204  ...  0.002882  0.009094  0.007748  0.003847   \n",
              "99  0.002897  0.002287  0.005233  ...  0.000072  0.002417  0.008347  0.003509   \n",
              "\n",
              "         95        96        97        98        99         100  \n",
              "0   0.744338  0.001491  0.004291  0.000343  0.004839  45.538701  \n",
              "1   0.743271  0.005223  0.002254  0.003261  0.001974  45.538701  \n",
              "2   0.745950  0.002701  0.001980  0.003601  0.009468  45.538701  \n",
              "3   0.742091  0.002588  0.003049  0.002851  0.000154  45.538701  \n",
              "4   0.740541  0.002000  0.003082  0.008175  0.007439  45.538701  \n",
              "..       ...       ...       ...       ...       ...        ...  \n",
              "95  0.744331  0.002065  0.003456  0.005854  0.004906  45.538701  \n",
              "96  0.737325  0.003319  0.005943  0.004717  0.004093  45.538701  \n",
              "97  0.736838  0.004420  0.007015  0.005209  0.008344  45.538701  \n",
              "98  0.741815  0.008631  0.006728  0.004986  0.001852  45.538701  \n",
              "99  0.743395  0.000612  0.002360  0.004410  0.003551  45.538701  \n",
              "\n",
              "[100 rows x 101 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1558cb0-bd2f-4296-9ae0-aeb8ae27ebf6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000736</td>\n",
              "      <td>0.003261</td>\n",
              "      <td>0.003091</td>\n",
              "      <td>0.005631</td>\n",
              "      <td>0.005303</td>\n",
              "      <td>0.002512</td>\n",
              "      <td>0.005898</td>\n",
              "      <td>0.000993</td>\n",
              "      <td>0.006528</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002851</td>\n",
              "      <td>0.006006</td>\n",
              "      <td>0.008011</td>\n",
              "      <td>0.008991</td>\n",
              "      <td>0.744338</td>\n",
              "      <td>0.001491</td>\n",
              "      <td>0.004291</td>\n",
              "      <td>0.000343</td>\n",
              "      <td>0.004839</td>\n",
              "      <td>45.538701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.005558</td>\n",
              "      <td>0.006796</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>0.004064</td>\n",
              "      <td>0.006750</td>\n",
              "      <td>0.003526</td>\n",
              "      <td>0.006443</td>\n",
              "      <td>0.008298</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.005633</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003693</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>0.003585</td>\n",
              "      <td>0.002004</td>\n",
              "      <td>0.743271</td>\n",
              "      <td>0.005223</td>\n",
              "      <td>0.002254</td>\n",
              "      <td>0.003261</td>\n",
              "      <td>0.001974</td>\n",
              "      <td>45.538701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.002769</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>0.001797</td>\n",
              "      <td>0.003745</td>\n",
              "      <td>0.008263</td>\n",
              "      <td>0.000747</td>\n",
              "      <td>0.003933</td>\n",
              "      <td>0.002002</td>\n",
              "      <td>0.008654</td>\n",
              "      <td>0.007585</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009344</td>\n",
              "      <td>0.007984</td>\n",
              "      <td>0.007712</td>\n",
              "      <td>0.005985</td>\n",
              "      <td>0.745950</td>\n",
              "      <td>0.002701</td>\n",
              "      <td>0.001980</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.009468</td>\n",
              "      <td>45.538701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.006754</td>\n",
              "      <td>0.002327</td>\n",
              "      <td>0.007948</td>\n",
              "      <td>0.004225</td>\n",
              "      <td>0.000736</td>\n",
              "      <td>0.008806</td>\n",
              "      <td>0.009342</td>\n",
              "      <td>0.009554</td>\n",
              "      <td>0.005927</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006932</td>\n",
              "      <td>0.001151</td>\n",
              "      <td>0.005495</td>\n",
              "      <td>0.009020</td>\n",
              "      <td>0.742091</td>\n",
              "      <td>0.002588</td>\n",
              "      <td>0.003049</td>\n",
              "      <td>0.002851</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>45.538701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005102</td>\n",
              "      <td>0.003130</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>0.005910</td>\n",
              "      <td>0.009319</td>\n",
              "      <td>0.005042</td>\n",
              "      <td>0.004585</td>\n",
              "      <td>0.002443</td>\n",
              "      <td>0.003127</td>\n",
              "      <td>0.002256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006578</td>\n",
              "      <td>0.009494</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.002139</td>\n",
              "      <td>0.740541</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.003082</td>\n",
              "      <td>0.008175</td>\n",
              "      <td>0.007439</td>\n",
              "      <td>45.538701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.001138</td>\n",
              "      <td>0.003412</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.006940</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.000787</td>\n",
              "      <td>0.009720</td>\n",
              "      <td>0.003857</td>\n",
              "      <td>0.003234</td>\n",
              "      <td>0.005187</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003192</td>\n",
              "      <td>0.005011</td>\n",
              "      <td>0.006775</td>\n",
              "      <td>0.007486</td>\n",
              "      <td>0.744331</td>\n",
              "      <td>0.002065</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>0.005854</td>\n",
              "      <td>0.004906</td>\n",
              "      <td>45.538701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.001178</td>\n",
              "      <td>0.007707</td>\n",
              "      <td>0.004206</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.003721</td>\n",
              "      <td>0.006909</td>\n",
              "      <td>0.009865</td>\n",
              "      <td>0.008238</td>\n",
              "      <td>0.009274</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005135</td>\n",
              "      <td>0.001427</td>\n",
              "      <td>0.009901</td>\n",
              "      <td>0.005763</td>\n",
              "      <td>0.737325</td>\n",
              "      <td>0.003319</td>\n",
              "      <td>0.005943</td>\n",
              "      <td>0.004717</td>\n",
              "      <td>0.004093</td>\n",
              "      <td>45.538701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.000805</td>\n",
              "      <td>0.006576</td>\n",
              "      <td>0.005944</td>\n",
              "      <td>0.005812</td>\n",
              "      <td>0.002645</td>\n",
              "      <td>0.004091</td>\n",
              "      <td>0.005904</td>\n",
              "      <td>0.001569</td>\n",
              "      <td>0.004554</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004521</td>\n",
              "      <td>0.002444</td>\n",
              "      <td>0.004916</td>\n",
              "      <td>0.006115</td>\n",
              "      <td>0.736838</td>\n",
              "      <td>0.004420</td>\n",
              "      <td>0.007015</td>\n",
              "      <td>0.005209</td>\n",
              "      <td>0.008344</td>\n",
              "      <td>45.538701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.000795</td>\n",
              "      <td>0.008041</td>\n",
              "      <td>0.007542</td>\n",
              "      <td>0.007155</td>\n",
              "      <td>0.004164</td>\n",
              "      <td>0.004607</td>\n",
              "      <td>0.008145</td>\n",
              "      <td>0.004014</td>\n",
              "      <td>0.003970</td>\n",
              "      <td>0.006204</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002882</td>\n",
              "      <td>0.009094</td>\n",
              "      <td>0.007748</td>\n",
              "      <td>0.003847</td>\n",
              "      <td>0.741815</td>\n",
              "      <td>0.008631</td>\n",
              "      <td>0.006728</td>\n",
              "      <td>0.004986</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>45.538701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.007825</td>\n",
              "      <td>0.006430</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.007625</td>\n",
              "      <td>0.008290</td>\n",
              "      <td>0.008100</td>\n",
              "      <td>0.009624</td>\n",
              "      <td>0.002897</td>\n",
              "      <td>0.002287</td>\n",
              "      <td>0.005233</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.002417</td>\n",
              "      <td>0.008347</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.743395</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.002360</td>\n",
              "      <td>0.004410</td>\n",
              "      <td>0.003551</td>\n",
              "      <td>45.538701</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 101 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1558cb0-bd2f-4296-9ae0-aeb8ae27ebf6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1558cb0-bd2f-4296-9ae0-aeb8ae27ebf6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1558cb0-bd2f-4296-9ae0-aeb8ae27ebf6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Transformer_GPT.acc(TestsetF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwknMt8fO-l_",
        "outputId": "6890e53d-fcd8-470d-e8ce-faa143b6a0f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89.40999762490145"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns83q7e70TAo"
      },
      "source": [
        "#Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuXVs5Uhkk49",
        "outputId": "5c5d07d5-15f6-49a7-c75b-9e4a81de72b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrRNjdpEk7Pv",
        "outputId": "41e09eff-bcd7-4ddc-ca28-017bf7a8d7cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbuild\u001b[0m/              \u001b[01;34mdocker\u001b[0m/     Makefile     \u001b[01;34mpyro_ppl.egg-info\u001b[0m/     setup.cfg\n",
            "CODE_OF_CONDUCT.md  \u001b[01;34mdocs\u001b[0m/       MANIFEST.in  README.md              setup.py\n",
            "CONTRIBUTING.md     \u001b[01;34mexamples\u001b[0m/   \u001b[01;34mprofiler\u001b[0m/    RELEASE-MANAGEMENT.md  \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdist\u001b[0m/               LICENSE.md  \u001b[01;34mpyro\u001b[0m/        \u001b[01;34mscripts\u001b[0m/               \u001b[01;34mtutorial\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS9ReDrG0a9B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "X=np.random.random((55000, 52))\n",
        "df= pd.read_csv('https://raw.githubusercontent.com/AMLab-Amsterdam/CEVAE/master/datasets/IHDP/csv/ihdp_npci_1.csv', header = None)\n",
        "\n",
        "df.dataframeName = 'data'\n",
        "cols =  [\"treatment\", \"y_factual\", \"y_cfactual\", \"mu0\", \"mu1\"] + [i for i in range(25)]\n",
        "\n",
        "df.columns = cols\n",
        "df = pd.concat([df]*100, ignore_index=True)\n",
        "\n",
        "# X=pd.DataFrame(X)\n",
        "\n",
        "class MyDataset(Dataset):\n",
        " \n",
        "  def __init__(self): \n",
        "    x=np.random.random((55000, 52))\n",
        "\n",
        "    self.x_test=torch.tensor(x,dtype=torch.float32)\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.x_test)\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    return self.x_test[idx],self.x_test[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihtv5_vH1L5_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga58dVBq06ov"
      },
      "outputs": [],
      "source": [
        "myDs=MyDataset()\n",
        "data_loader=DataLoader(myDs,batch_size=10,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EwOAGId19fn"
      },
      "outputs": [],
      "source": [
        "X=[]\n",
        "for i in data_loader:\n",
        "  X.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23Es7snl2LPF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U7pAnVm-ozZ"
      },
      "source": [
        "#Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOulsDyU-mo9"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ns83q7e70TAo",
        "2U7pAnVm-ozZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}